{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Su-Han-Kim/programming/blob/main/CS_124.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2T5yRtnRgxWg"
      },
      "source": [
        "# 1. Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Regular Expressions, Text Normalization, Edit Distance"
      ],
      "metadata": {
        "id": "Y6Qxpyofu3yz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Regular Expressions\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "wZ31V82xy0Mh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- regular expression: A formal language for specifying text strings\n"
      ],
      "metadata": {
        "id": "F38-bSxgvQgX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1. Disjuctions\n",
        "- Letters insides []\n",
        "- ranges [A-Za-z0-9가-힣]"
      ],
      "metadata": {
        "id": "W39QXuzUkWui"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2. Negation in Disjunctions\n",
        "- **Negations [^]:** Carat means negation only when first in []\n"
      ],
      "metadata": {
        "id": "qb1bvP9Iy2o1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 1.3. More Disjunctions\n",
        "- **|:** The pipe for disjuction(or)\n",
        "\n"
      ],
      "metadata": {
        "id": "zZQBn3h5y663"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.4. ? * + .\n",
        "- **?:** Optional previous char\n",
        "- **\\*:** 0 or more of previous char\n",
        "- **+:** 1 or more of previous char\n",
        "- **.:** meaning any char\n",
        "\n"
      ],
      "metadata": {
        "id": "AfoQCq86zAy0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.5. Anchors\n",
        "- **^:** beginning of the line\n",
        "- **<code>&#36;</code>:** end of the line\n",
        "ex) Find me all instances of the word 'the' in a text\n",
        "- the: Misses capitalized examples   \n",
        "  -> **Type 2 error(False negatives)** -> Increasing coverage or recall\n",
        "- [tT]he: Incorrectly returns other or theology  \n",
        "  -> **Type 1 error(False positive)** -> Incresing accuracy or precision\n",
        "- **[^a-zA-Z][tT]he[^a-zA-Z]**\n",
        "- In NLP we are always dealing with these kinds of errors\n",
        "- Reducing the error rate for an application often involves two antagonistic efforts\n",
        "\n"
      ],
      "metadata": {
        "id": "lsWv_Q6kzDfc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Regular Expression Substitution"
      ],
      "metadata": {
        "id": "e6Fj0pcJzS6-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1. Substitutions\n",
        "- ex) s/colour/color\n",
        "\n"
      ],
      "metadata": {
        "id": "GeLEXJsezL38"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2. Capture Groups\n",
        "- Say we want to put angles around all numbers:\n",
        "- Use parens to \"capture\" a pattern into a numbered register(1,2,3...)\n",
        "- Use \\1 to refer to the content of the register  \n",
        "  ex) s/([0-9]+)/<\\1>/\n",
        " \n"
      ],
      "metadata": {
        "id": "H5Ztj7LbzXyG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.2.1. Capture groups: multiple registers\n",
        "- ex) /the (.\\*)er they (.\\*), the \\1er we \\2/   \n",
        "  -> the faster they ran, the faster we ran -> MATCH  \n",
        "  -> the faster they ran, the faster we **ate** -> UNMATCH\n",
        "  \n"
      ],
      "metadata": {
        "id": "9anCYNaozbhQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3. But suppose we don't want to capture\n",
        "- Parentheses have a double function: grouping terms, and capturing\n",
        "- Non-capturing groups: add a ?:after paren:  \n",
        "- ex) /(?:some|a few) (people|cats) like some \\1/  \n",
        "  -> (?:some|a few): grouping but not capture\n",
        "  -> some cats like some cats -> MATCH     \n",
        "  -> some cats like some some -> UNMATCH\n"
      ],
      "metadata": {
        "id": "UIFVp6lmzeVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 2.4. Lookahead assertions\n",
        "- (?= pattern) is true if pattern matched, but is **zero-width; doesn't advance character pointer**\n",
        "- (!= pattern) true if pattern does not match\n",
        "- ex) How to match at the beginning of the line, any single word that doesn't start with \"Volcano\": /^(?!Volcano)[A-Za-z]+/\n",
        "\n"
      ],
      "metadata": {
        "id": "7ZfplyELzk0E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.5. Simple Application: ELIZA\n",
        "- Early NLP system that imitated a Rogerian psychotherapist: Joseph Weizenbaum 1966\n",
        "- How ELIZA works  \n",
        "  ex) s/.\\* I'M (depressed|sad).\\*/I AM SORRY TO HEAR YOU ARE \\1/  \n",
        "  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;s/.\\* I AM (depressed|sad).\\*/WHY DO YOU THINK YOU ARE \\1/\n",
        " \n"
      ],
      "metadata": {
        "id": "lbB4VHZqzqQf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ELIZA\n",
        "import re\n",
        "# s/.\\* I AM (depressed|sad).\\*/WHY DO YOU THINK YOU ARE \\1/\n",
        "\n",
        "a = input()\n",
        "\n",
        "print(re.sub('I AM (?P<feel>[a-z]+)', 'WHY DO YOU THINK YOU ARE \\\\1', a))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SaMEHWTL0G_z",
        "outputId": "54abdd3d-c6b0-4acd-d747-1c1a15b46d70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I AM sad\n",
            "WHY DO YOU THINK YOU ARE sad\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Words and Corpora\n",
        "\n"
      ],
      "metadata": {
        "id": "EkT0NsmyzshX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1. How many words in a sentence?\n",
        "- ex) I do **uh(pauses) main-(fragment)** mainly business data processing  \n",
        "  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-> Fragments, filled pauses\n",
        "- ex) Seuss's **cat** in the hat is different from other **cats!**  \n",
        "  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-> Lemma: same stem, part of speech, rough word sense  \n",
        "  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-> Wordform: the full inflected surface form -> cat and cats are same lemma, different wordforms\n",
        "- ex) they lay back on the San Francisco grass and looked at the stars and their  \n",
        "  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-> Type: an element of the vocabulary  \n",
        "  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-> Token an instance of that type in running text  \n",
        "  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-> 14 or 15 tokens, 11 or 12 or 13 types  \n",
        "- **N:** Number of tokens  \n",
        "- **V:** Vocabulary = set of types, **|V|** is size of vocabulary\n",
        "- Heaps Law = Herden's Law = $|V| = kN^{\\beta}$ where often .67 < b < .75\n",
        "  ex) vocabulary size grows with > spuare root of the number of word tokens\n",
        "- Zipf's law: 기울기를 보고 stop word를 가려낼 때 쓴다.\n",
        "\n"
      ],
      "metadata": {
        "id": "uuERJpxbzuWS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2. Corpora\n",
        "- A text is produced by (a specific writers, at a specific time, in a  specific variety, of a specific language, for a specific function)\n",
        "\n"
      ],
      "metadata": {
        "id": "Uj2ka8Cmzw0c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3. Corpora vary along dimension like"
      ],
      "metadata": {
        "id": "YGS65WFwz30W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.4. Corpus datasheets\n",
        "\n"
      ],
      "metadata": {
        "id": "O-RH80G1z2Xu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Word tokenization\n"
      ],
      "metadata": {
        "id": "DmnQYOHCz8Zl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 본 분석에 들어가기 전에 언어학적 베이스를 가지고 (어떤 데이터든 적용 가능하게끔)전처리를 해주기 위함"
      ],
      "metadata": {
        "id": "NUX31FzIve14"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1. Text Nomalization\n",
        "- Every NLP task requires text normaliztion:\n",
        "    1. Tokenizing(segmenting) words\n",
        "    2. Normalizing word formats\n",
        "    3. Segmenting sentence\n"
      ],
      "metadata": {
        "id": "-2wA_7o1z-44"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 4.2. Space-based tokenization\n",
        "- A very simple way to tokenize\n",
        "    - For languages that use space characters between words  \n",
        "      ex) Arabic, Cyrillic, Greek, Latin...\n",
        "    - Segment off a token between instance of space\n"
      ],
      "metadata": {
        "id": "WJ8_dOk00Amp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3. Issues in Tokenization\n",
        "- Can't just blindly remove punctuation: [:punct:]\n",
        "- Clitic: a word that doesn't stand on its own: ex) are in **we're**\n",
        "- When should multiword expressions(MWE) be word: ex) New york"
      ],
      "metadata": {
        "id": "MnWC_e3J0aeV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.4. Tokenization in languages without spaces\n",
        "- Many languages(like Chinese) don'y use space to separate words!\n",
        "- How do we decide where the token boundaries should be? "
      ],
      "metadata": {
        "id": "RQ2Xoqti0f5x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.5. Word tokenization / segmentation\n",
        "- So in Chinese it's common to just treat each character(zi) as a token\n",
        "- In other languages(like Thai and japanese), more complex word segmentation is required\n",
        "    - The standard algorithms are neural sequence models trained by supervised machine learning"
      ],
      "metadata": {
        "id": "RtMyrutm0hr1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Byte Pair Encoding"
      ],
      "metadata": {
        "id": "57eNmqF20jzD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.1. Another option for text tokenization\n",
        "- Instead of white-space or single character segmentation\n"
      ],
      "metadata": {
        "id": "cqrzowxA0oTb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 5.2. Subword tokenization\n",
        "- Three commom algorithms:\n",
        "    1. Byte-pair Encoding(BPE)\n",
        "    2. Unigram language modeling tokenizaton\n",
        "    3. WordPiece ex) BERT\n",
        "- All have 2 parts\n",
        "    1. A token **learner** that takes a raw training corpus and induces a vocabulary(a set of tokens)\n",
        "    2. A token **segmenter** that takes a raw test sentence and tokenizes it according to that vocabulary"
      ],
      "metadata": {
        "id": "81ohUAO10p2n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.3. Byte Pair Encoding\n",
        "- Let vocabulary be the set of all indiviual characters  \n",
        "    = {A, B, C, D,...a, b, c, d...}\n",
        "- Repeat\n",
        "    1. Choose the twa symbols that are most frequently adjacent in the traning corpus(say 'A', 'B')\n",
        "    2. Add a new merged symbol 'AB' to the vocabulary\n",
        "    3. Replace every adjacent 'A''B' in the corpus with 'AB'\n",
        "- Until k merges have been done"
      ],
      "metadata": {
        "id": "6qxTMcwH0r6T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.4. BPE token learner algorithm \n",
        "    def BPE(string C, merges K)(C, K):  \n",
        "        V = all unique characters in C  \n",
        "        for i in k:  \n",
        "            t<sub>L</sub>, t<sub>R</sub> = Most frequent pair adjacent tokens til k times  \n",
        "            t<sub>new</sub> = t<sub>L</sub> + t<sub>R</sub>\n",
        "            V = V + t<sub>new</sub>\n",
        "            Replace each occurrence of t<sub>L</sub>, t<sub>R</sub> in C with t<sub>new</sub>\n",
        "            return V"
      ],
      "metadata": {
        "id": "vG07CRx70wNX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.5. BPE Addendum\n",
        "- Most subword algorithms are run inside space-separated tokens.\n",
        "- So we commonly first add a special end-of-word symbol '_' before space in training corpus \n",
        "- Next, separate into letters"
      ],
      "metadata": {
        "id": "D6bpFLoU050E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.6. BPE token learner"
      ],
      "metadata": {
        "id": "SD3_YmWv09LV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.7. BPE token segmenter algorithm\n",
        "- On the test data, run each merge learned from the training data:\n",
        "    1. Greedily\n",
        "    2. In the order we learned them\n",
        "    3. (test frequencies don't play a role)"
      ],
      "metadata": {
        "id": "7_bsmIsi0_Ij"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.8. Properties of BPE tokens\n",
        "- Usually include frequent words\n",
        "- And frequent subwords\n",
        "    - Which are often morphemes like -est or -er"
      ],
      "metadata": {
        "id": "hvIlSRM01Akl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Word Normalization"
      ],
      "metadata": {
        "id": "OTwW_Wx61NYR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.1. Word Normalization\n",
        "- Putting words/tokens in a  standard format\n",
        "    - U.S.A or USA\n",
        "    - uhhuh or uh-huh\n",
        "    - Fed or fed\n",
        "    - am, is, be, are(same lemma)"
      ],
      "metadata": {
        "id": "34GK_AKO1OXF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.2. Case folding\n",
        "- Applications liks IR: reduce all letters to lower cas\n",
        "    - Since users tend to use lower case\n",
        "    - Possible exception: upper case in mid-sentence?  \n",
        "      ex) Fed or fed, General Motors\n",
        "- For sentiment analysis, MT, Information extraction\n",
        "    - Case is helpful (US versus us is important)"
      ],
      "metadata": {
        "id": "emkCPuGm1P5V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.3. Lemmatization\n",
        "- Represent all words as their lemma, their shared root = dictionary headword form:"
      ],
      "metadata": {
        "id": "trwSkWzZ1UT8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.4. Lemmatization is done by Morphological Parsing"
      ],
      "metadata": {
        "id": "QR5e_ATe1aV1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.5. Stemming\n",
        "- Reduce terms to stems, chopping off affixes crudely"
      ],
      "metadata": {
        "id": "loppGyF51bOP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.6. Porter Stemmer\n",
        "- Based on a series of rewrite rules run in series\n",
        "    - A cascade, in which output of each pass fed to next pass  \n",
        "      ex) ATIONAL -> ATE (e.g. relational -> relate)  \n",
        "      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ING -> e if stem contains vowel (e.g. motoring -> motor)  \n",
        "      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;SSES -> SS (e.g. grasses -> grass)"
      ],
      "metadata": {
        "id": "gX2mb4NW1bQ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.7. Dealing with complex morphology is necessary for many language"
      ],
      "metadata": {
        "id": "xFOfpMb91bTt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.8. Sentence Segmenatation\n",
        "- ex) Sentence boundary or Dr. or .02\\% or 4.3\n",
        "- Common algorithm: Tokenize first: use rules or ML to classify a period as either (a) part of the word or (b) a sentence boundary\n",
        "    - An abbreviation dictionary can help\n",
        "- Sentence segmentation can then often be done by rules based on this tokenization\n",
        "https://www.youtube.com/c/SKplanetTacademy/search?query=BERT"
      ],
      "metadata": {
        "id": "CkfS592f1bWW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Defining Minimum Edit Distance "
      ],
      "metadata": {
        "id": "_dgsUuly1bZP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.1. How similar are two strings? \n",
        "- Spell correction ex) graffe -> graf? graft? grail? giraffe? \n",
        "- Computational Biology: Align two sequences of nucleotides  \n",
        "    ex) AGGCTATC/TAGCTATC -> -AGGCTATC/TAG-CTATC\n",
        "- Also for Machine Translation, Information Extraction, Speech Recognition"
      ],
      "metadata": {
        "id": "sG0j1FJK1bb0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.2. Edit distance\n",
        "- The minimum edit distance between two strings\n",
        "- Is the minimum number of editing operations ex) Insertion, Deletion, Substitution\n",
        "- Needed to transform one into the other\n",
        "    ex) intention -> execution -> d/s/s/-/i/s/-/-/-/-"
      ],
      "metadata": {
        "id": "FPle5Ejr1beO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.3. Alignment in Computational Biology\n",
        "- Given a sequence of bases\n",
        "    ex) AGGTATCA/TAGCTATC\n",
        "- An alignment\n",
        "    ex) -AGGCTATC/TAG-CTATC"
      ],
      "metadata": {
        "id": "SLLrXGbF1bgx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.4. Other uses of Edit Distance in NLP \n",
        "- Evaluating Machine Translation and speech recognition  \n",
        "    - R: Spokesman confirms \\_\\_\\_ senior government adviser was shot \\_\\_\\_\\_  \n",
        "    - H: Spokesman said\\_\\_\\_\\_     the senior \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_ adviser wad shot dead\n",
        "- Named Entity Extraction and Entity Coreference\n",
        "    - **IBM Inc.** announced today\n",
        "    - **IBM** profits\n",
        "    - **Stanford President John Hennessy** announced yesterday\n",
        "    - for **stanford University President John Hennessy**"
      ],
      "metadata": {
        "id": "ZTZ7cIWG1byD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.5. How to find the Minimim Edit distance?\n",
        "- Searching for a path(sequence of edits) from the start string to the final string: \n",
        "    - Initial state: the word we're transforming\n",
        "    - Operatiors: insert, delete, substitute\n",
        "    - Goal state: the word we're trying to get to\n",
        "    - path cost: what we want to minimize: the number of edits  \n",
        "      ex) intention -> entention(Sub)  \n",
        "      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;intention -> -ntention(Del)  \n",
        "      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;intention -> eintention(Ins) "
      ],
      "metadata": {
        "id": "rmzlGU5D1b0n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.6. Minimum Edit as Search\n",
        "- But the space of all edit sequences is Huge!\n",
        "    - We can't afford to navigate naively\n",
        "    - Lots of distinct paths wind up at the same state\n",
        "        - We don't have to keep track of all of them\n",
        "        - Just the shortest path to each of those revisited states"
      ],
      "metadata": {
        "id": "YZB24rN-1b3G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.7. Defining Minimum Edit Distance\n",
        "- For two string ex) X of length n/Y of length m\n",
        "- We define D(i,j)\n",
        "    - The edit distance between X[1:i] and Y[1:j]\n",
        "        ex) The first i characters of X and the first j characters of Y\n",
        "    - The edit distance between X and Y is thus D(n,m)"
      ],
      "metadata": {
        "id": "s7b5L6Uy1b5p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Computing Minimum Edit Distance"
      ],
      "metadata": {
        "id": "uRtFHBc-11LE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.1. Dynamic Programming for Minimum Edit Distance\n",
        "- Dynamic programming: A tabular computation of D(n,m)\n",
        "- Solving problem by combining solutions to subproblems.\n",
        "- Bottom-up\n",
        "    - We compute D(i,j) for small i, j\n",
        "    - And compute larger D(i,j) based on previously computed smaller values\n",
        "    - i.e. compute D(i,j) for all i(0 < i < n) and j(0 < j < m)"
      ],
      "metadata": {
        "id": "JcE4LYcu11UF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.2. Defining Minimum Edit Distance(Levenshtein)\n",
        "- Initialization:\n",
        "    - D(i,0) = i -> i개의 string에서 공집합으로 바꾸는 최소 거리는 i \n",
        "    - D(0,j) = j -> 공집합에서 j개의 string으로 바꾸는 최소 거리는 j \n",
        "- Recurrence Relation: see script below"
      ],
      "metadata": {
        "id": "ckuYnga011bZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Minimum Edit Distance Algorithm \n",
        "#referance: https://blog.naver.com/ndb796/220870218783\n",
        "#From languages to information에서는 Substitution을 2번 edit한 것으로 취급하기 때문에 결과가 다르게 나옴\n",
        "#잘 활용하면 문장단위로도 편집거리를 구할 수 있을 듯\n",
        "from IPython.display import display\n",
        "import pandas as pd # 데이터를 저장하고 처리하는 패키지\n",
        "\n",
        "str1 = 'intention'\n",
        "str2 = 'execution'         \n",
        "\n",
        "\n",
        "def m_med(str1, str2):\n",
        "    matrix = []\n",
        "    for i in range(len(str2)+1):\n",
        "        matrix.append(list(map(int, [0 for j in range(len(str1)+1)])))\n",
        "    matrix[0] = list(range(len(str1)+1)) # 공집합과 문자열의 거리1\n",
        "    for i in range(len(str2)+1): # 공집합과 문자열의 거리2\n",
        "        matrix[i][0] = i\n",
        "    for i in range(1,len(matrix)): #str2의 길이\n",
        "        for j in range(1, len(matrix[i])): #str1의 길이\n",
        "            if str1[j-1] == str2[i-1]:\n",
        "                matrix[i][j] = matrix[i-1][j-1]\n",
        "            else:\n",
        "                if min(matrix[i-1][j-1], matrix[i][j-1], matrix[i-1][j]) == matrix[i-1][j-1]:\n",
        "                   matrix[i][j] = min(matrix[i-1][j-1], matrix[i][j-1], matrix[i-1][j])+2             \n",
        "                else:\n",
        "                   matrix[i][j] = min(matrix[i-1][j-1], matrix[i][j-1], matrix[i-1][j])+1             \n",
        "    return matrix\n",
        "\n",
        "def c_med(str1, str2):\n",
        "    return f'{str2}에서 {str1}사이의 최소 편집거리는 {m_med(str1, str2)[-1][-1]}입니다.'\n",
        "\n",
        "str1_list = [0]\n",
        "str2_list = [0]\n",
        "for i in str1:\n",
        "    str1_list.append(i)\n",
        "for i in str2:\n",
        "    str2_list.append(i)\n",
        "\n",
        "display(pd.DataFrame(m_med(str1, str2), index=str1_list, columns=str2_list))\n",
        "\n",
        "print(c_med(str1, str2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "KwJdqSOx2Jhf",
        "outputId": "420ab57f-1423-4422-e218-9768d210ae5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-27796464-44dc-40c3-a5fb-65a8d458c51a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>e</th>\n",
              "      <th>x</th>\n",
              "      <th>e</th>\n",
              "      <th>c</th>\n",
              "      <th>u</th>\n",
              "      <th>t</th>\n",
              "      <th>i</th>\n",
              "      <th>o</th>\n",
              "      <th>n</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>i</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>n</th>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>t</th>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>9</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>e</th>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>9</td>\n",
              "      <td>10</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>n</th>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>9</td>\n",
              "      <td>10</td>\n",
              "      <td>11</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>t</th>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>9</td>\n",
              "      <td>8</td>\n",
              "      <td>9</td>\n",
              "      <td>10</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>i</th>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>9</td>\n",
              "      <td>10</td>\n",
              "      <td>9</td>\n",
              "      <td>8</td>\n",
              "      <td>9</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>o</th>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>9</td>\n",
              "      <td>10</td>\n",
              "      <td>11</td>\n",
              "      <td>10</td>\n",
              "      <td>9</td>\n",
              "      <td>8</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>n</th>\n",
              "      <td>9</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>9</td>\n",
              "      <td>10</td>\n",
              "      <td>11</td>\n",
              "      <td>10</td>\n",
              "      <td>9</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-27796464-44dc-40c3-a5fb-65a8d458c51a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-27796464-44dc-40c3-a5fb-65a8d458c51a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-27796464-44dc-40c3-a5fb-65a8d458c51a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   0  e  x  e   c   u   t   i   o   n\n",
              "0  0  1  2  3   4   5   6   7   8   9\n",
              "i  1  2  3  4   3   4   5   6   7   8\n",
              "n  2  3  4  5   4   5   6   7   8   9\n",
              "t  3  4  5  6   5   6   7   8   9  10\n",
              "e  4  5  6  7   6   7   8   9  10  11\n",
              "n  5  6  7  8   7   8   9  10  11  12\n",
              "t  6  7  8  7   8   9   8   9  10  11\n",
              "i  7  6  7  8   9  10   9   8   9  10\n",
              "o  8  7  8  9  10  11  10   9   8   9\n",
              "n  9  8  7  8   9  10  11  10   9   8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "execution에서 intention사이의 최소 편집거리는 8입니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Backtrace for Computing Alignments"
      ],
      "metadata": {
        "id": "hmfo6-MA11jr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9.1. Computing alignments\n",
        "- Edit distance isn't sufficient\n",
        "    - We often need to **align** each character of the two strings to each other\n",
        "- We do thos by keeping a \"backtrace\"\n",
        "- Every time we enter a cell, remember where we came from\n",
        "- When we reach the end\n",
        "    - Trace back the path from the upper right corner to read off the alignment"
      ],
      "metadata": {
        "id": "-_9HPbR011ot"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-zNXIUTGgxWm",
        "outputId": "97aeea14-0357-456c-c143-d7b0a16af03f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "아래는 execution에서 intention사이의 최소 편집거리를 구하는 과정입니다.\n",
            "u를 n로 변경합니다\n",
            "c를 e로 변경합니다\n",
            "e를 t로 변경합니다\n",
            "x를 n로 변경합니다\n",
            "e를 i로 변경합니다\n"
          ]
        }
      ],
      "source": [
        "def s_med(str1, str2):\n",
        "    b = len(str1)\n",
        "    a = len(str2)\n",
        "    print(f'아래는 {str2}에서 {str1}사이의 최소 편집거리를 구하는 과정입니다.')\n",
        "    while b > 0 and a > 0:\n",
        "        c = min(m_med(str1, str2)[a-1][b-1], m_med(str1, str2)[a][b-1], m_med(str1, str2)[a-1][b])\n",
        "        if c == m_med(str1, str2)[a-1][b-1]:\n",
        "            if c == m_med(str1, str2)[a][b]:\n",
        "                b = b-1\n",
        "                a = a-1\n",
        "            else:\n",
        "                print(f'{str2[a-1]}를 {str1[b-1]}로 변경합니다')\n",
        "                b = b-1\n",
        "                a = a-1\n",
        "        elif c == m_med(str1, str2)[a-1][b]:\n",
        "            if c == m_med(str1, str2)[a][b]:\n",
        "                a = a-1\n",
        "            else: \n",
        "                print(f'{str2[a-1]}를 삭제합니다')\n",
        "                a = a-1\n",
        "        elif c == m_med(str1, str2)[a][b-1]:\n",
        "            if c == m_med(str1, str2)[a][b]:\n",
        "                b = b-1      \n",
        "            else:\n",
        "                print(f'{str1[b-1]}를 삽입합니다')      \n",
        "                b = b-1      \n",
        "                \n",
        "s_med(str1, str2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. Weighted Minimum Edit Distance\n",
        "\n"
      ],
      "metadata": {
        "id": "qrogZivH3Xro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10.1. Weighted Edit Distance\n",
        "- Why would we add weight to the computation?\n",
        "  - Spell Correction: some letters are more likely to be mistyped than others\n",
        "  - Biology: certain kinds of deletions or insertions are more likely than others\n",
        "- 이전에 거리를 계산할 때 더했던 1대신 실제로 편집(삽입, 삭제, 대체)할 때 나오는 특별한 거리를 더해준다  "
      ],
      "metadata": {
        "id": "QRsiAe-g3f_V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10.2. Where did the name, dynamic programming, come from?\n"
      ],
      "metadata": {
        "id": "Ky7dbiLU4oSh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11. Minimum Edit Distance in Computational Biology"
      ],
      "metadata": {
        "id": "J1j7X_nw4yO5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11.1. Why sequence alignment?\n",
        "- Comparing genes or regions from different species\n",
        "  - to find important regions\n",
        "  - determine function\n",
        "  - uncover evolutionary forces\n",
        "- Assembling fragments to sequence DNA\n",
        "- Compare indiviuals to looking for mutuations\n"
      ],
      "metadata": {
        "id": "m6HTMKqRq81A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11.2. Alignments in two fields\n",
        "- In Natural Languiage Processing\n",
        "  - We generally talk about **distance**(minimized)\n",
        "    - And weights\n",
        "- In Computational Biology\n",
        "  - We generally talk about similarity(maximized)\n",
        "    - And scores\n",
        "    \n",
        "    "
      ],
      "metadata": {
        "id": "XYXs3ApPsZkX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11.3. The Needleman-Wunsch Algorithm"
      ],
      "metadata": {
        "id": "6juVBhNxlyoz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11.4. The Needleman-Wunsch Matrix"
      ],
      "metadata": {
        "id": "IPLHWypznJ1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Needleman-Wunsch Matrix Algorithm \n",
        "#referance: https://enjoybioinfo.blogspot.com/2020/10/needleman-wunsch-algorithm.html\n",
        "from IPython.display import display\n",
        "import pandas as pd # 데이터를 저장하고 처리하는 패키지\n",
        "\n",
        "seq1 = 'AGTCG'\n",
        "seq2 = 'ATGG'         \n",
        "\n",
        "\n",
        "def m_med(str1, str2):\n",
        "    matrix = []\n",
        "    for i in range(len(str2)+1):\n",
        "        matrix.append(list(map(int, [0 for j in range(len(str1)+1)])))\n",
        "    matrix[0] = list(range(len(str1)+1)) # 공집합과 문자열의 거리1\n",
        "    for i in matrix[0]:\n",
        "        n = i * -1\n",
        "        matrix[0][i] = n\n",
        "    for i in range(len(str2)+1): # 공집합과 문자열의 거리2\n",
        "        matrix[i][0] = i*-1\n",
        "    for i in range(1,len(matrix)): #str2의 길이\n",
        "        for j in range(1, len(matrix[i])): #str1의 길이\n",
        "            if str1[j-1] == str2[i-1]:\n",
        "                matrix[i][j] = max((matrix[i-1][j-1])+1, (matrix[i][j-1])-1, (matrix[i-1][j])-1)\n",
        "            else:\n",
        "                matrix[i][j] = max(matrix[i-1][j-1], (matrix[i][j-1])-1, (matrix[i-1][j])-1)            \n",
        "    return matrix\n",
        "\n",
        "str1_list = [0]\n",
        "str2_list = [0]\n",
        "for i in seq1:\n",
        "    str1_list.append(i)\n",
        "for i in seq2:\n",
        "    str2_list.append(i)\n",
        "\n",
        "display(pd.DataFrame(m_med(seq1, seq2), index=str2_list, columns=str1_list))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "H865dW6zBFpk",
        "outputId": "094ac47b-98b1-4971-a85e-792f823c7547"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-c56117cb-e24e-419b-b6da-9621bc5a65ae\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>A</th>\n",
              "      <th>G</th>\n",
              "      <th>T</th>\n",
              "      <th>C</th>\n",
              "      <th>G</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-2</td>\n",
              "      <td>-3</td>\n",
              "      <td>-4</td>\n",
              "      <td>-5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>A</th>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-2</td>\n",
              "      <td>-3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T</th>\n",
              "      <td>-2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>G</th>\n",
              "      <td>-3</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>G</th>\n",
              "      <td>-4</td>\n",
              "      <td>-2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c56117cb-e24e-419b-b6da-9621bc5a65ae')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c56117cb-e24e-419b-b6da-9621bc5a65ae button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c56117cb-e24e-419b-b6da-9621bc5a65ae');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   0  A  G  T  C  G\n",
              "0  0 -1 -2 -3 -4 -5\n",
              "A -1  1  0 -1 -2 -3\n",
              "T -2  0  1  1  0 -1\n",
              "G -3 -1  1  1  1  1\n",
              "G -4 -2  0  1  1  2"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11.5. A variant of the basic algorithm\n",
        "- Maybe it is OK to have an unlimited # of gaps in the beginning and end:\n",
        "  - If so, we don't want to penalize gaps at the ends\n",
        "  "
      ],
      "metadata": {
        "id": "idLRfl1ynXsz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11.6. Different types of overlaps"
      ],
      "metadata": {
        "id": "kgnrVynZn0k9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11.7. The overlap Detection variant"
      ],
      "metadata": {
        "id": "ajr5ioLdoPFi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11.8. The local Alignment Problem\n",
        "- Given two strings x = x<sub>1</sub>...x<sub>M</sub>, y = y<sub>1</sub>...y<sub>N</sub> \n",
        "- Find substrings x', y' whose similarity(optimal global alignment value) is maximum"
      ],
      "metadata": {
        "id": "_ITDen2doVFb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11.9. The smith-Waterman algorithm\n",
        "- Idea: Ignore badly aligning regions\n",
        "- Modifications to Needleman-Wunsch "
      ],
      "metadata": {
        "id": "CvQyeMrmpPkT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#The smith-Waterman algorithm\n",
        "#referance: https://en.wikipedia.org/wiki/Smith%E2%80%93Waterman_algorithm\n",
        "from IPython.display import display\n",
        "import pandas as pd # 데이터를 저장하고 처리하는 패키지\n",
        "\n",
        "seq1 = 'TGTTACGG'\n",
        "seq2 = 'GGTTGACTA'         \n",
        "\n",
        "\n",
        "def m_med(str1, str2):\n",
        "    matrix = []\n",
        "    for i in range(len(str2)+1):\n",
        "        matrix.append(list(map(int, [0 for j in range(len(str1)+1)])))\n",
        "    for i in range(1,len(matrix)): #str2의 길이\n",
        "        for j in range(1, len(matrix[i])): #str1의 길이\n",
        "            if str1[j-1] == str2[i-1]:\n",
        "                matrix[i][j] = max((matrix[i-1][j-1])+3, (matrix[i][j-1])-2, (matrix[i-1][j])-2, 0)\n",
        "            else:\n",
        "                matrix[i][j] = max((matrix[i-1][j-1])-3, (matrix[i][j-1])-2, (matrix[i-1][j])-2, 0)\n",
        "    return matrix\n",
        "\n",
        "str1_list = [0]\n",
        "str2_list = [0]\n",
        "for i in seq1:\n",
        "    str1_list.append(i)\n",
        "for i in seq2:\n",
        "    str2_list.append(i)\n",
        "\n",
        "display(pd.DataFrame(m_med(seq1, seq2), index=str2_list, columns=str1_list))"
      ],
      "metadata": {
        "id": "uCZ4Y9XO3W1s",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "20580165-417a-4d55-806d-2f17d68c562f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-0dfa0dee-93c3-4468-b708-aad3d7d18794\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>T</th>\n",
              "      <th>G</th>\n",
              "      <th>T</th>\n",
              "      <th>T</th>\n",
              "      <th>A</th>\n",
              "      <th>C</th>\n",
              "      <th>G</th>\n",
              "      <th>G</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>G</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>G</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>9</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>G</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>A</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>10</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>C</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>13</td>\n",
              "      <td>11</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>11</td>\n",
              "      <td>10</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>A</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>9</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0dfa0dee-93c3-4468-b708-aad3d7d18794')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0dfa0dee-93c3-4468-b708-aad3d7d18794 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0dfa0dee-93c3-4468-b708-aad3d7d18794');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   0  T  G  T  T   A   C   G  G\n",
              "0  0  0  0  0  0   0   0   0  0\n",
              "G  0  0  3  1  0   0   0   3  3\n",
              "G  0  0  3  1  0   0   0   3  6\n",
              "T  0  3  1  6  4   2   0   1  4\n",
              "T  0  3  1  4  9   7   5   3  2\n",
              "G  0  1  6  4  7   6   4   8  6\n",
              "A  0  0  4  3  5  10   8   6  5\n",
              "C  0  0  2  1  3   8  13  11  9\n",
              "T  0  3  1  5  4   6  11  10  8\n",
              "A  0  1  0  3  2   7   9   8  7"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. N-gram Language Models"
      ],
      "metadata": {
        "id": "kOX-EFfNuP7_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Introduction to N-grams"
      ],
      "metadata": {
        "id": "-Wc8ppFSuXj3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1. Probabilistic Language Models\n",
        "- goal of Language Model: assign a probability to a sentence\n",
        "  - Machine Translation:\n",
        "    - P(**high** wind tonite) > P(**large** wind tonite)\n",
        "  - Spell Correction\n",
        "    - The office is about fifteen **minuets** from my house  \n",
        "      P(about fifteen **minutes** from) > P(about fifteen **minuets** from)\n",
        "  - Speech Recognition\n",
        "    - P(I saw a van) >> P(eyes awe of an)\n",
        "\n",
        "- Goal: compute the probability of a sentence or sequence of words:  \n",
        "  P(W) = P(W1, W2, W3, ... Wn) 전체 corpus에서 W라는 문장 or 단어 배열이 올 확률\n",
        "- Related task: Probability of an upcoming word:  \n",
        "  P(W5|W1, W2, W3, W4)  \n",
        "  ex) P(the|its water is so transparent that)  \n",
        "  전체 문서에서 its water is so transparent that 뒤에 'the'가 올 확률은 its water is so transparent가 오는 모든 경우에서 its water is so transparent that the를 나눈 것  \n",
        "  **-> C(its water is so transparent that the) / C(its water is so transparent that)** \n",
        "\n",
        "- A model that computes either of these is called a **language model**\n",
        "- Better: **the grammer** But **language model** or **LM** is standard\n"
      ],
      "metadata": {
        "id": "e7WK3JWPwK4G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2. How to compute P(W)\n",
        "- How to compute this joint probability\n",
        "  - ex) P(its, water, is so transparent, that)\n",
        "- Intuition: let's rely on the Chain Rule of Probability"
      ],
      "metadata": {
        "id": "Yjh_amjvyxw3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3. Reminder: The Chain Rule\n",
        "- Recall the definition of conditional probabilities\n",
        "- More variables:  \n",
        "  P(W1, W2, W3, W4) = P(W1)P(W2|W1)P(W3|W1, W2)P(W4|W1, W2, W3)  \n",
        "  P(W) = 전체 corpus에서 W1이 나올 확률 \\* W1 뒤에 W2가 나올 확률 \\* W1, W2 뒤에 W3이 나올 확률 ... \\* W1, ... , Wn-1뒤에 Wn이 나올 확률    \n",
        "  $$P(W) = \\prod_{i}P(W_i|W_1W_2...W_{i-1})$$"
      ],
      "metadata": {
        "id": "Eh-Zwaq2zL-0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.4. The Chain Rule applied to compute joint probability of words in sentence\n",
        "- ex) P(its water is so transparent) = P(its) \\* P(water|its) \\* P(is|its water) \\* P(so|its water is) \\* P(transparent|its water is so)"
      ],
      "metadata": {
        "id": "R2pKLAHb5hqy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.5. How to estimate these probabilities\n",
        "- Could we just count and divide?   \n",
        "  ex) P(the|its water is so transparent that) = **C(its water is so transparent that the) / C(its water is so transparent that)** \n",
        "- No! Too many possible sentences\n",
        "- We'll never see enough data for estimating these"
      ],
      "metadata": {
        "id": "--F4XpSj8MQ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.6. Markov Assumption\n",
        "- Simplifying assumption: $$P(the|its\\,water\\,is\\,so\\,transparent\\,that) \\approx  P(the|that) $$\n",
        "- Or maybe: $$P(the|its\\,water\\,is\\,so\\,transparent\\,that) \\approx  P(the|transparent\\,that) $$\n",
        "- Markov Assumption: $$P(W) \\approx \\prod_{i}P(W_i|W_{i-k}..W_{i-1})$$\n",
        "- In other words, we approximate each component in the product: $$P(W_i|W_1W_2...W_{i-1}) \\approx P(W_i|W_{i-k}..W_{i-1}) $$\n"
      ],
      "metadata": {
        "id": "JUiFuxUd-Gq-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.7. simplest case: Unigram model\n",
        "$$P(W) \\approx \\prod_{i}P(W_i)$$\n",
        "ex) P(its water is so transparent that) = P(its) \\* P(water) \\* P(is) \\* P(so) \\* P(transparent) \\* P(that)  \n",
        "그러나 곱하기는 교환법칙이 성립하기 때문에 'its water is so transparent that'이나 'water is so transparent that its'나 같은 경우로 취급됨  \n",
        "referance : https://jiho-ml.com/weekly-nlp-14/\n",
        "- Some automatically generated sentences from a unigram model\n"
      ],
      "metadata": {
        "id": "Jk6ABmHN_eui"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.8. Bigram model\n",
        "- Condition on the previous word\n",
        "$$P(W_i|W_1W_2...W_{i-1}) \\approx P(W_i|W_{i-1})$$\n",
        "ex) P(its water is so transparent that) = P(its|\\<start\\>) \\* P(water|its) \\* P(is|water) \\* P(so|is) \\* P(transparent|so) \\* P(that|transparent)"
      ],
      "metadata": {
        "id": "GVUMw5JXBJae"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.9. N-gram models \n",
        "- We can extend to trigrams, 4-grams, 5-grams\n",
        "- In general this is an insufficient model of language\n",
        "  - because language has **long-distance dependencies**\n",
        "  - \"the computer which I han just put into the machine room an the fifth floor crashed\""
      ],
      "metadata": {
        "id": "xBScEWo_Bpxq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'its water is so transparent that'\n",
        "\n",
        "def gram(text, n):\n",
        "    text = text.split()\n",
        "    text_gram = []\n",
        "    for i in range(len(text)-n+1):\n",
        "        text_gram.append(text[i:i+n])\n",
        "    return text_gram\n",
        "\n",
        "print(gram(text, 2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FOBaUxQrFevH",
        "outputId": "36961cb8-4cd6-4e01-8ac0-6ec6c0d5e649"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['its', 'water'], ['water', 'is'], ['is', 'so'], ['so', 'transparent'], ['transparent', 'that']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Fc5vVTxFCd3K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Estimating N gram Probabilities"
      ],
      "metadata": {
        "id": "D5pxplScjue3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1. Estimating bigram probabilities\n",
        "\n",
        "- The Maximum Likelihood Estimate\n",
        "  $$P(W_i|W_{i-1}) = \\frac{count(W_{i-1},W_i)}{count(W_{i-1})}$$\n",
        "\n",
        "  $$P(W_i|W_{i-1}) = \\frac{c(W_{i-1},W_i)}{c(W_{i-1})}$$\n",
        "  \n",
        "  ex) I am Sam / Sam I am / I do not like green eggs and ham  \n",
        "    -> P(I|\\<s>) = 2/3, P(Sam|\\<s>) = 1/3, P(am|I) = 2/3, P(Sam|am) = 1/2"
      ],
      "metadata": {
        "id": "K7Dgo5aQj39G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2. More examples: Berkeley Restaurant Project sentences"
      ],
      "metadata": {
        "id": "yTSUX-SVm7gf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display\n",
        "import pandas as pd # 데이터를 저장하고 처리하는 패키지\n",
        "import math # log 써야 해서 필요함 \n",
        "\n",
        "str1 = 'can you tell me about any good cantonese restaurants close by'\n",
        "str2 = \"mid priced thai food is what i'm looking for\"\n",
        "str3 = 'tell me about chez panisse'\n",
        "str4 = 'can you give me a listing of the kinds of food that are available'\n",
        "str5 = \"i'm looking for good place to eat breakfast\"\n",
        "str6 = 'when is caffe venezia open during the day' \n",
        "str_list = [str1.split(), str2.split(), str3.split(), str4.split(), str5.split(), str6.split()]\n",
        "\n",
        "def bow(li):\n",
        "    a = []\n",
        "    for i in range(len(li)):\n",
        "        a.extend(li[i])\n",
        "    a = list(set(a))\n",
        "    return a\n",
        "\n",
        "\n",
        "\n",
        "def n_gram(bow, str_list, window):\n",
        "    a = []\n",
        "    for i in range(len(str_list)): \n",
        "      a.extend(str_list[i])\n",
        "    matrix = []\n",
        "    for i in range(len(bow)):\n",
        "        matrix.append(list(map(int, [0 for j in range(len(bow))]))) # matrix 생성      \n",
        "    for i in range(len(matrix)): \n",
        "        for j in range(len(matrix[i])): # matrix 범위 지정\n",
        "            for k in range(len(str_list)):\n",
        "                for l in range(len(str_list[k])): # str_list 범위 지정\n",
        "                    if bow[j] == str_list[k][l]: # bow속 단어가 str_list어디에 있는 지 찾기\n",
        "                        for x in range(l, l+window): # context window 내에 str_list에서 찾는 단어가 있는 지 찾기\n",
        "                            if x < 0 or x == l or x >= len(str_list[k]): #범위를 벗어나면 넘기기\n",
        "                                continue\n",
        "                            elif str_list[k][x] == bow[i]:\n",
        "                                matrix[i][j] = matrix[i][j] + 1/a.count(str_list[k][l])\n",
        "    return matrix\n",
        "\n",
        "print(bow(str_list))\n",
        "n_gram = n_gram(bow(str_list), str_list, 2)\n",
        "\n",
        "display(pd.DataFrame(n_gram, index=bow(str_list), columns=bow(str_list))) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1kiiya77npnO",
        "outputId": "90db59c8-be95-43bb-89ba-f42da18b702d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['eat', 'looking', 'me', 'can', 'caffe', 'available', 'mid', 'during', 'about', 'chez', 'kinds', 'any', 'you', 'tell', 'place', 'open', 'cantonese', 'good', 'by', 'give', 'a', 'listing', 'restaurants', \"i'm\", 'are', 'of', 'breakfast', 'to', 'that', 'when', 'thai', 'what', 'is', 'for', 'venezia', 'close', 'day', 'food', 'priced', 'the', 'panisse']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-0cbe1352-84c7-47bf-af49-2ad2c8597d25\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>eat</th>\n",
              "      <th>looking</th>\n",
              "      <th>me</th>\n",
              "      <th>can</th>\n",
              "      <th>caffe</th>\n",
              "      <th>available</th>\n",
              "      <th>mid</th>\n",
              "      <th>during</th>\n",
              "      <th>about</th>\n",
              "      <th>chez</th>\n",
              "      <th>kinds</th>\n",
              "      <th>any</th>\n",
              "      <th>you</th>\n",
              "      <th>tell</th>\n",
              "      <th>place</th>\n",
              "      <th>open</th>\n",
              "      <th>cantonese</th>\n",
              "      <th>good</th>\n",
              "      <th>by</th>\n",
              "      <th>give</th>\n",
              "      <th>a</th>\n",
              "      <th>listing</th>\n",
              "      <th>restaurants</th>\n",
              "      <th>i'm</th>\n",
              "      <th>are</th>\n",
              "      <th>of</th>\n",
              "      <th>breakfast</th>\n",
              "      <th>to</th>\n",
              "      <th>that</th>\n",
              "      <th>when</th>\n",
              "      <th>thai</th>\n",
              "      <th>what</th>\n",
              "      <th>is</th>\n",
              "      <th>for</th>\n",
              "      <th>venezia</th>\n",
              "      <th>close</th>\n",
              "      <th>day</th>\n",
              "      <th>food</th>\n",
              "      <th>priced</th>\n",
              "      <th>the</th>\n",
              "      <th>panisse</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>eat</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>looking</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>me</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>can</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>caffe</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>available</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mid</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>during</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>about</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>chez</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>kinds</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>any</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>you</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>tell</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>place</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>open</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cantonese</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>good</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>by</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>give</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>a</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>listing</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>restaurants</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>i'm</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>are</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>of</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>breakfast</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>to</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>that</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>when</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>thai</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>what</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>is</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>for</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>venezia</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>close</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>day</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>food</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>priced</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>the</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>panisse</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0cbe1352-84c7-47bf-af49-2ad2c8597d25')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0cbe1352-84c7-47bf-af49-2ad2c8597d25 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0cbe1352-84c7-47bf-af49-2ad2c8597d25');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "             eat  looking        me  can  ...  food  priced  the  panisse\n",
              "eat          0.0      0.0  0.000000  0.0  ...   0.0     0.0  0.0        0\n",
              "looking      0.0      0.0  0.000000  0.0  ...   0.0     0.0  0.0        0\n",
              "me           0.0      0.0  0.000000  0.0  ...   0.0     0.0  0.0        0\n",
              "can          0.0      0.0  0.000000  0.0  ...   0.0     0.0  0.0        0\n",
              "caffe        0.0      0.0  0.000000  0.0  ...   0.0     0.0  0.0        0\n",
              "available    0.0      0.0  0.000000  0.0  ...   0.0     0.0  0.0        0\n",
              "mid          0.0      0.0  0.000000  0.0  ...   0.0     0.0  0.0        0\n",
              "during       0.0      0.0  0.000000  0.0  ...   0.0     0.0  0.0        0\n",
              "about        0.0      0.0  0.666667  0.0  ...   0.0     0.0  0.0        0\n",
              "chez         0.0      0.0  0.000000  0.0  ...   0.0     0.0  0.0        0\n",
              "kinds        0.0      0.0  0.000000  0.0  ...   0.0     0.0  0.5        0\n",
              "any          0.0      0.0  0.000000  0.0  ...   0.0     0.0  0.0        0\n",
              "you          0.0      0.0  0.000000  1.0  ...   0.0     0.0  0.0        0\n",
              "tell         0.0      0.0  0.000000  0.0  ...   0.0     0.0  0.0        0\n",
              "place        0.0      0.0  0.000000  0.0  ...   0.0     0.0  0.0        0\n",
              "open         0.0      0.0  0.000000  0.0  ...   0.0     0.0  0.0        0\n",
              "cantonese    0.0      0.0  0.000000  0.0  ...   0.0     0.0  0.0        0\n",
              "good         0.0      0.0  0.000000  0.0  ...   0.0     0.0  0.0        0\n",
              "by           0.0      0.0  0.000000  0.0  ...   0.0     0.0  0.0        0\n",
              "give         0.0      0.0  0.000000  0.0  ...   0.0     0.0  0.0        0\n",
              "a            0.0      0.0  0.333333  0.0  ...   0.0     0.0  0.0        0\n",
              "listing      0.0      0.0  0.000000  0.0  ...   0.0     0.0  0.0        0\n",
              "restaurants  0.0      0.0  0.000000  0.0  ...   0.0     0.0  0.0        0\n",
              "i'm          0.0      0.0  0.000000  0.0  ...   0.0     0.0  0.0        0\n",
              "are          0.0      0.0  0.000000  0.0  ...   0.0     0.0  0.0        0\n",
              "of           0.0      0.0  0.000000  0.0  ...   0.0     0.0  0.0        0\n",
              "breakfast    1.0      0.0  0.000000  0.0  ...   0.0     0.0  0.0        0\n",
              "to           0.0      0.0  0.000000  0.0  ...   0.0     0.0  0.0        0\n",
              "that         0.0      0.0  0.000000  0.0  ...   0.5     0.0  0.0        0\n",
              "when         0.0      0.0  0.000000  0.0  ...   0.0     0.0  0.0        0\n",
              "thai         0.0      0.0  0.000000  0.0  ...   0.0     1.0  0.0        0\n",
              "what         0.0      0.0  0.000000  0.0  ...   0.0     0.0  0.0        0\n",
              "is           0.0      0.0  0.000000  0.0  ...   0.5     0.0  0.0        0\n",
              "for          0.0      1.0  0.000000  0.0  ...   0.0     0.0  0.0        0\n",
              "venezia      0.0      0.0  0.000000  0.0  ...   0.0     0.0  0.0        0\n",
              "close        0.0      0.0  0.000000  0.0  ...   0.0     0.0  0.0        0\n",
              "day          0.0      0.0  0.000000  0.0  ...   0.0     0.0  0.5        0\n",
              "food         0.0      0.0  0.000000  0.0  ...   0.0     0.0  0.0        0\n",
              "priced       0.0      0.0  0.000000  0.0  ...   0.0     0.0  0.0        0\n",
              "the          0.0      0.0  0.000000  0.0  ...   0.0     0.0  0.0        0\n",
              "panisse      0.0      0.0  0.000000  0.0  ...   0.0     0.0  0.0        0\n",
              "\n",
              "[41 rows x 41 columns]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3. Bigram estimates of sentence probabilities\n",
        "\n",
        "- P(to|want) = 2/3\n",
        "- P(to|food) = 0: grammartic reason "
      ],
      "metadata": {
        "id": "7B7r2LYbuVGh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.4. Practical Issues\n",
        "\n",
        "- We do everything in log space\n",
        "  - Avoid underflow\n",
        "  - (also adding is faster than multiplying)\n",
        "  $$p_1 \\times p_2 \\times p_3 \\times p_4 = logp_1 \\times logp_2 \\times logp_3 \\times logp_4$$"
      ],
      "metadata": {
        "id": "dT_lZzIau3I9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.5. Language Modeling Toolkits\n",
        "- SRILM: http://www.speech.sri.com/projects/srilm/ "
      ],
      "metadata": {
        "id": "RhIXv18yxGRV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.6. Google N-gram Release, August 2006\n",
        "- https://books.google.com/ngrams\n"
      ],
      "metadata": {
        "id": "MBG_4fZvxPq1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Evaluation and Perplexity"
      ],
      "metadata": {
        "id": "42SQ1W0nAqWx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1. Evaluation: How good is our model?\n",
        "- referance: https://settlelib.tistory.com/55?category=901586 \n",
        "- Does our language model prefer good sentences to bad ones? \n",
        "  - Assign higher probability to \"real\" or \"frequently observed\" sentences\n",
        "    - Than \"ungrammatical\" or \"rarely observed\" sentences?\n",
        "- We train parameters of our model on a **training set**\n",
        "- We test the model's performance on data we haven't seen\n",
        "  - A **test set** is an unseen dataset that is different from our training set, totally unused.\n",
        "  - An **evaluation metric** tells us how well our model does on the test set.\n",
        "  "
      ],
      "metadata": {
        "id": "F5cC66ijA4T6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2. Extrinsic evaluation of N-gram models\n",
        "\n",
        "- Best evaluation for comparing models A and B\n",
        "  - Put each model in a Task\n",
        "    - spelling corrector, speech recognizer, MT system\n",
        "    - How many misspelled words corrected properly\n",
        "    - How many words translated correctly\n",
        "  Compare accuracy for A and B -> **extrinsic evaluation**\n",
        "  "
      ],
      "metadata": {
        "id": "Ivg2vIxpVClQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3. Difficulty of extrinsic(in-vivo) evaluation of N-gram models\n",
        "\n",
        "- Extrinsic evaluation\n",
        "  - Time-consuming;can take days or weeks\n",
        "- So\n",
        "  - Sometime use **intrinsic** evaluation: **perplexity**\n",
        "  - Bad approximation\n",
        "    - unless the test data looks **just** like the training data\n",
        "    - So **generally only useful in pilot experiments** \n",
        "  - But is helpful to think about\n"
      ],
      "metadata": {
        "id": "OZvsEU3mWAgU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.4. Intuition of Perplexity \n",
        "\n",
        "- The Shannon Game\n",
        "  - How well can we predict the next word?  \n",
        "    ex) I always order pizza with cheese and \\_\\_\\_\\_ / I saw a \\_\\_\\_\n",
        "  - Unigrams are terrible at this game(Why?)\n",
        "- A better model of a text\n",
        "  - is one which assigns a higher probability to the word that actually occurs\n",
        "  "
      ],
      "metadata": {
        "id": "hfxIOFiqdqt4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.5. Perplexity \n",
        "\n",
        "- The best language model is one that best predicts an unseen test set\n",
        "  - Gives the highest P(sentence)\n",
        "- Perplexity is the probability of the test set, normalized by the number of words:\n",
        "  - PPL: $$PP(W) = \\sqrt[N]{\\prod_{i}^N\\frac{1}{P(W_1W_2...W_N)}}$$\n",
        "  - Chain rule: $$PP(W) = \\sqrt[N]{\\prod_{i}^N\\frac{1}{P(W_i|W_1W_2...W_{i-1})}}$$\n",
        "  - For bigrams: $$PP(W) = \\sqrt[N]{\\prod_{i}^N\\frac{1}{P(W_i|W_{i-1})}}$$\n",
        "  - Minimizing perplexity is the same as maximizing probability"
      ],
      "metadata": {
        "id": "JR2s7WQRe-kL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.6. The Shannon Game intuition for perplexity\n",
        "- From Josh Goodman\n",
        "- How hard is the task of recognizing digit '0-9'\n",
        "  - Perplexity 10\n",
        "- How hard is recognizing (30,000) names at Microsoft\n",
        "  - Perplexity = 30,000\n",
        "- If a system has to recognize (A call-routing phone system get 120K calls and has to recognize)\n",
        "  - Operator(1 in 4)\n",
        "  - Sales(1 in 4)\n",
        "  - Technical Support(1 in 4)\n",
        "  - 30,000 names (1 in 120,000 each)\n",
        "  - Perplexity is 54\n",
        "- Perplexity is weighted equivalent branching\n",
        "- To get the perplexity of this sequence of length 120K  \n",
        "  1) multiply 120K probabilities(90K of which are 1/4 and 30K of which are 1/120K)  \n",
        "  2) take the inverse 120,000th root: Perp = (1/4 * ... 1/120K\\*...)^(-1/120K)  \n",
        "    Can be arithmetically simplified to just N = 4: operator(1/4), sales(1/4), tech support(1/4), and 30,000 names(1/120000):   \n",
        "    PPL = (1/4 \\*1/4\\*1/4 \\*1/120K\\)^(-1/4) = 52.6\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "B-75PeHfj81U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.7. Perplexity as branching factor\n",
        "- Let's suppose a sentence consisting of random digits\n",
        "- What is the perplexity of this sentence according to a model that assign P=1/10 to each digit? \n",
        "$$PP(W) = {P(W_1W_2...W_N)^{-\\frac{1}{N}}} = \\left(\\frac{1}{10}^N\\right)^{-\\frac{1}{N}} = \\left(\\frac{1}{10}\\right)^{-1} = 10$$ \n"
      ],
      "metadata": {
        "id": "wPBu9TlMoom9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.8. Lower perplexity = better model\n",
        "\n",
        "- Training 38 million words, test 1.5 million words, WSJ  \n",
        "ex) Unigram = 962, Bigram = 170, Trigram = 109"
      ],
      "metadata": {
        "id": "C_ha-XxVr4BQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Generalization and zeros"
      ],
      "metadata": {
        "id": "3iIHW_VtsgrB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1. The Shannon Visualization Method\n",
        "- Choose a random bigram: (\\<s\\>, w) according to its probability\n",
        "- Now choose a random bigram (w, x)  according to its probability\n",
        "- And so on until we choose \\</s\\>\n",
        "- Then string the words together"
      ],
      "metadata": {
        "id": "lIljqtazslb9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2. Shakespeare as corpus\n",
        "- N = 884,647 tokens, V = 29,066\n",
        "- Shakespeare produced 300,000 bigram types out of V<sup>2</sup>=844 million possible bigrams.\n",
        "  - So 99.96% of the possible bigrams were never seen(have zero entries in the table)\n",
        "- Quadrigrams worse: What's coming out look like Shakespeare because it is Shakespeare"
      ],
      "metadata": {
        "id": "C24cJEFNtP9r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3. The perils of overfitting\n",
        "- N-grams only work well for word prediction if the best corpus looks like the training corpus\n",
        "  - In real life, it often doesn't\n",
        "  - We need to train robust models that generalize!\n",
        "  - One kind of generalization: Zeros!\n",
        "    - Things that don't ever occur in the training set\n",
        "      - But occur in the test set"
      ],
      "metadata": {
        "id": "8GqgfTZvuXkz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.4. Zeros\n",
        "\n",
        "- Training set: denied the {allegation, reports, claims, request}\n",
        "- P(\"offer\"|denied the) = 0\n",
        "- Test set: denied the {offer, loan}\n"
      ],
      "metadata": {
        "id": "bQ0YVgUAvVYj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.5. Zero probability bigrams\n",
        "- Bigrams with zero probability\n",
        "  - mean that we will assign 0 probability to the test set!\n",
        "- And hence we cannot compute perplexity (can't divide by 0)"
      ],
      "metadata": {
        "id": "XN-GTm1vwFiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Smoothing Add One"
      ],
      "metadata": {
        "id": "0r5R_csy7GqG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.1. The intuition of smoothing(from Dan Klein)\n",
        "- referance: https://heiwais25.github.io/nlp/2019/10/06/Language-model-2/ \n",
        "- When we have sparse statistics:  \n",
        "- Steal probability mass to generalize better\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hrNuD9Ol7MrD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.2. Add-one estimation\n",
        "- Also called Laplace smoothing\n",
        "- Pretend we saw each word one more time than we did\n",
        "- Just add one to all the counts!\n",
        "- MLE estimate: $$P_{MLE}(W_i|W_{i-1}) = \\frac{c(W_{i-1},W_i)}{c(W_{i-1})}$$\n",
        "- Add-1 estimate: $$P_{Add-1}(W_i|W_{i-1}) = \\frac{c(W_{i-1},W_i)+1}{c(W_{i-1})+V}$$\n"
      ],
      "metadata": {
        "id": "sSS54-QpqDph"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.3. Maximum Likelihood Estimates\n",
        "- The Maximum likelihood estimate\n",
        "  - of some parameter of a model M from a training set T\n",
        "  - maximizes the likelihood of the training set T given the model M\n",
        "- Suppose the word \"bagel\" occurs 400 times in a corpus of a million words\n",
        "- What is the probability that a random word from some other text will be \"bagel\"? \n",
        "- MLE estimate is 400/1,000,000 = .0004\n",
        "- This may be a bad estimate for some other corpus\n",
        "  - But it is the estimate that makes it **most likely** that \"bagel\" will occur 400 times in a million word corpus.\n",
        "  \n"
      ],
      "metadata": {
        "id": "W8AmhSETrX5y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.4. Laplace-smoothed bigrams"
      ],
      "metadata": {
        "id": "VgKuscTFtZqB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display\n",
        "import pandas as pd # 데이터를 저장하고 처리하는 패키지\n",
        "import math # log 써야 해서 필요함 \n",
        "\n",
        "w_list = ['i', 'want', 'to', 'eat', 'chinese', 'food', 'lunch', 'spend']\n",
        "R = [[5, 827, 0, 9, 0, 0, 0, 2],[2, 0, 608, 1, 6, 6, 5, 1],[2, 0, 4, 686, 2, 0, 6, 211],[0, 0, 2, 0, 16, 2, 42, 0],[1, 0, 0, 0, 0, 82, 1, 0],[15, 0, 15, 0, 1, 4, 0, 0], [2, 0, 0, 0, 0, 1, 0, 0],[1, 0, 1, 0, 0, 0, 0, 0]]\n",
        "display(pd.DataFrame(R, index=w_list, columns=w_list)) \n",
        "\n",
        "for i in  range(len(R)):\n",
        "  for j in range(len(R[i])):\n",
        "    p_i = 0\n",
        "    for k in range(len(R)):\n",
        "      p_i = p_i + R[k][j]\n",
        "    R[i][j] = R[i][j]/p_i\n",
        "\n",
        "display(pd.DataFrame(R, index=w_list, columns=w_list)) \n",
        "\n",
        "# #perplexity\n",
        "# pp = 1\n",
        "# for i in  range(len(R)):\n",
        "#   for j in range(len(R[i])):\n",
        "#     pp = pp * R[i][j]\n",
        "# pp = 1/pp ** (1/(len(R) * len(R[0])))\n",
        "# print(pp) #Zero-division error\n",
        "\n",
        "#add-1 smoothing\n",
        "for i in  range(len(R)):\n",
        "  for j in range(len(R[i])):\n",
        "    p_i = 0\n",
        "    for k in range(len(R)):\n",
        "      p_i = p_i + R[k][j]\n",
        "    R[i][j] = (R[i][j]+1)/(p_i+len(R[i]))\n",
        "\n",
        "\n",
        "display(pd.DataFrame(R, index=w_list, columns=w_list)) \n",
        "\n",
        "#perplexity\n",
        "pp = 1\n",
        "for i in  range(len(R)):\n",
        "  for j in range(len(R[i])):\n",
        "    pp = pp * R[i][j]\n",
        "pp = 1/pp ** (1/(len(R) * len(R[0])))\n",
        "print(pp)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 905
        },
        "id": "q5joViWVL6r6",
        "outputId": "1a25d087-e68f-4e3c-f33d-c812f61d3fc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "          i  want   to  eat  chinese  food  lunch  spend\n",
              "i         5   827    0    9        0     0      0      2\n",
              "want      2     0  608    1        6     6      5      1\n",
              "to        2     0    4  686        2     0      6    211\n",
              "eat       0     0    2    0       16     2     42      0\n",
              "chinese   1     0    0    0        0    82      1      0\n",
              "food     15     0   15    0        1     4      0      0\n",
              "lunch     2     0    0    0        0     1      0      0\n",
              "spend     1     0    1    0        0     0      0      0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-bbbcbb46-0d6e-482a-8428-aff6338397ac\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>i</th>\n",
              "      <th>want</th>\n",
              "      <th>to</th>\n",
              "      <th>eat</th>\n",
              "      <th>chinese</th>\n",
              "      <th>food</th>\n",
              "      <th>lunch</th>\n",
              "      <th>spend</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>i</th>\n",
              "      <td>5</td>\n",
              "      <td>827</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>want</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>608</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>to</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>686</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>211</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>eat</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>16</td>\n",
              "      <td>2</td>\n",
              "      <td>42</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>chinese</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>82</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>food</th>\n",
              "      <td>15</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>lunch</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>spend</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bbbcbb46-0d6e-482a-8428-aff6338397ac')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-bbbcbb46-0d6e-482a-8428-aff6338397ac button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-bbbcbb46-0d6e-482a-8428-aff6338397ac');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                i  want        to       eat   chinese      food     lunch  \\\n",
              "i        0.178571   1.0  0.000000  0.012931  0.000000  0.000000  0.000000   \n",
              "want     0.086287   0.0  0.965079  0.001456  0.240000  0.063158  0.092593   \n",
              "to       0.094052   0.0  0.174177  0.999979  0.103950  0.000000  0.122218   \n",
              "eat      0.000000   0.0  0.104497  0.000000  0.922512  0.022456  0.971889   \n",
              "chinese  0.051656   0.0  0.000000  0.000000  0.000000  0.941602  0.457310   \n",
              "food     0.814750   0.0  0.869880  0.000000  0.441216  0.663656  0.000000   \n",
              "lunch    0.473337   0.0  0.000000  0.000000  0.000000  0.371627  0.000000   \n",
              "spend    0.370555   0.0  0.321168  0.000000  0.000000  0.000000  0.000000   \n",
              "\n",
              "            spend  \n",
              "i        0.009346  \n",
              "want     0.004717  \n",
              "to       0.999933  \n",
              "eat      0.000000  \n",
              "chinese  0.000000  \n",
              "food     0.000000  \n",
              "lunch    0.000000  \n",
              "spend    0.000000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-95b7bca8-251a-4097-baf5-e94dfdbcfd37\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>i</th>\n",
              "      <th>want</th>\n",
              "      <th>to</th>\n",
              "      <th>eat</th>\n",
              "      <th>chinese</th>\n",
              "      <th>food</th>\n",
              "      <th>lunch</th>\n",
              "      <th>spend</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>i</th>\n",
              "      <td>0.178571</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.012931</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.009346</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>want</th>\n",
              "      <td>0.086287</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.965079</td>\n",
              "      <td>0.001456</td>\n",
              "      <td>0.240000</td>\n",
              "      <td>0.063158</td>\n",
              "      <td>0.092593</td>\n",
              "      <td>0.004717</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>to</th>\n",
              "      <td>0.094052</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.174177</td>\n",
              "      <td>0.999979</td>\n",
              "      <td>0.103950</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.122218</td>\n",
              "      <td>0.999933</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>eat</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.104497</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.922512</td>\n",
              "      <td>0.022456</td>\n",
              "      <td>0.971889</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>chinese</th>\n",
              "      <td>0.051656</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.941602</td>\n",
              "      <td>0.457310</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>food</th>\n",
              "      <td>0.814750</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.869880</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.441216</td>\n",
              "      <td>0.663656</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>lunch</th>\n",
              "      <td>0.473337</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.371627</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>spend</th>\n",
              "      <td>0.370555</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.321168</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-95b7bca8-251a-4097-baf5-e94dfdbcfd37')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-95b7bca8-251a-4097-baf5-e94dfdbcfd37 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-95b7bca8-251a-4097-baf5-e94dfdbcfd37');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                i      want        to       eat   chinese      food     lunch  \\\n",
              "i        0.117047  0.222222  0.095833  0.112369  0.103011  0.099379  0.103691   \n",
              "want     0.108545  0.121622  0.186606  0.109883  0.126393  0.104622  0.112087   \n",
              "to       0.109079  0.119849  0.120402  0.216865  0.113844  0.098007  0.114897   \n",
              "eat      0.099552  0.118152  0.113885  0.118496  0.198055  0.099255  0.202040   \n",
              "chinese  0.103667  0.116525  0.103010  0.116855  0.111327  0.187086  0.162103   \n",
              "food     0.177977  0.114964  0.190594  0.115281  0.158483  0.172872  0.115011   \n",
              "lunch    0.154119  0.113464  0.109511  0.113769  0.113493  0.150186  0.113510   \n",
              "spend    0.148320  0.112022  0.142968  0.112315  0.112050  0.112216  0.112066   \n",
              "\n",
              "            spend  \n",
              "i        0.111975  \n",
              "want     0.110207  \n",
              "to       0.216863  \n",
              "eat      0.118497  \n",
              "chinese  0.116856  \n",
              "food     0.115282  \n",
              "lunch    0.113770  \n",
              "spend    0.112316  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e2f83a20-3a79-47f4-8da1-6b987eb066f2\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>i</th>\n",
              "      <th>want</th>\n",
              "      <th>to</th>\n",
              "      <th>eat</th>\n",
              "      <th>chinese</th>\n",
              "      <th>food</th>\n",
              "      <th>lunch</th>\n",
              "      <th>spend</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>i</th>\n",
              "      <td>0.117047</td>\n",
              "      <td>0.222222</td>\n",
              "      <td>0.095833</td>\n",
              "      <td>0.112369</td>\n",
              "      <td>0.103011</td>\n",
              "      <td>0.099379</td>\n",
              "      <td>0.103691</td>\n",
              "      <td>0.111975</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>want</th>\n",
              "      <td>0.108545</td>\n",
              "      <td>0.121622</td>\n",
              "      <td>0.186606</td>\n",
              "      <td>0.109883</td>\n",
              "      <td>0.126393</td>\n",
              "      <td>0.104622</td>\n",
              "      <td>0.112087</td>\n",
              "      <td>0.110207</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>to</th>\n",
              "      <td>0.109079</td>\n",
              "      <td>0.119849</td>\n",
              "      <td>0.120402</td>\n",
              "      <td>0.216865</td>\n",
              "      <td>0.113844</td>\n",
              "      <td>0.098007</td>\n",
              "      <td>0.114897</td>\n",
              "      <td>0.216863</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>eat</th>\n",
              "      <td>0.099552</td>\n",
              "      <td>0.118152</td>\n",
              "      <td>0.113885</td>\n",
              "      <td>0.118496</td>\n",
              "      <td>0.198055</td>\n",
              "      <td>0.099255</td>\n",
              "      <td>0.202040</td>\n",
              "      <td>0.118497</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>chinese</th>\n",
              "      <td>0.103667</td>\n",
              "      <td>0.116525</td>\n",
              "      <td>0.103010</td>\n",
              "      <td>0.116855</td>\n",
              "      <td>0.111327</td>\n",
              "      <td>0.187086</td>\n",
              "      <td>0.162103</td>\n",
              "      <td>0.116856</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>food</th>\n",
              "      <td>0.177977</td>\n",
              "      <td>0.114964</td>\n",
              "      <td>0.190594</td>\n",
              "      <td>0.115281</td>\n",
              "      <td>0.158483</td>\n",
              "      <td>0.172872</td>\n",
              "      <td>0.115011</td>\n",
              "      <td>0.115282</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>lunch</th>\n",
              "      <td>0.154119</td>\n",
              "      <td>0.113464</td>\n",
              "      <td>0.109511</td>\n",
              "      <td>0.113769</td>\n",
              "      <td>0.113493</td>\n",
              "      <td>0.150186</td>\n",
              "      <td>0.113510</td>\n",
              "      <td>0.113770</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>spend</th>\n",
              "      <td>0.148320</td>\n",
              "      <td>0.112022</td>\n",
              "      <td>0.142968</td>\n",
              "      <td>0.112315</td>\n",
              "      <td>0.112050</td>\n",
              "      <td>0.112216</td>\n",
              "      <td>0.112066</td>\n",
              "      <td>0.112316</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e2f83a20-3a79-47f4-8da1-6b987eb066f2')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e2f83a20-3a79-47f4-8da1-6b987eb066f2 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e2f83a20-3a79-47f4-8da1-6b987eb066f2');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7.97288016771782\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.5. Reconstitued counts\n",
        "\n",
        "$$c^*(W_i|W_{i-1}) = \\frac{[c(W_{i-1},W_i)+1]\\times c(W_{n-1})}{c(W_{i-1})+V}$$\n"
      ],
      "metadata": {
        "id": "Mo_28gGDtWJm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w_list = ['i', 'want', 'to', 'eat', 'chinese', 'food', 'lunch', 'spend']\n",
        "R = [[5, 827, 0, 9, 0, 0, 0, 2],[2, 0, 608, 1, 6, 6, 5, 1],[2, 0, 4, 686, 2, 0, 6, 211],[0, 0, 2, 0, 16, 2, 42, 0],[1, 0, 0, 0, 0, 82, 1, 0],[15, 0, 15, 0, 1, 4, 0, 0], [2, 0, 0, 0, 0, 1, 0, 0],[1, 0, 1, 0, 0, 0, 0, 0]]\n",
        "display(pd.DataFrame(R, index=w_list, columns=w_list)) \n",
        "\n",
        "#Reconstitued counts\n",
        "for i in  range(len(R)):\n",
        "  for j in range(len(R[i])):\n",
        "    p_i = 0\n",
        "    for k in range(len(R)):\n",
        "      p_i = p_i + R[k][j]\n",
        "    R[i][j] = (R[i][j]+1)*p_i/(p_i+len(R[i]))\n",
        "\n",
        "display(pd.DataFrame(R, index=w_list, columns=w_list)) "
      ],
      "metadata": {
        "id": "o4E6Ev4-dqON",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 583
        },
        "outputId": "c516e5e1-bc70-48e2-9fa0-ab4f60989bff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "          i  want   to  eat  chinese  food  lunch  spend\n",
              "i         5   827    0    9        0     0      0      2\n",
              "want      2     0  608    1        6     6      5      1\n",
              "to        2     0    4  686        2     0      6    211\n",
              "eat       0     0    2    0       16     2     42      0\n",
              "chinese   1     0    0    0        0    82      1      0\n",
              "food     15     0   15    0        1     4      0      0\n",
              "lunch     2     0    0    0        0     1      0      0\n",
              "spend     1     0    1    0        0     0      0      0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-876549d1-17b7-4f9e-a9ef-58d06a40849a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>i</th>\n",
              "      <th>want</th>\n",
              "      <th>to</th>\n",
              "      <th>eat</th>\n",
              "      <th>chinese</th>\n",
              "      <th>food</th>\n",
              "      <th>lunch</th>\n",
              "      <th>spend</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>i</th>\n",
              "      <td>5</td>\n",
              "      <td>827</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>want</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>608</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>to</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>686</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>211</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>eat</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>16</td>\n",
              "      <td>2</td>\n",
              "      <td>42</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>chinese</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>82</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>food</th>\n",
              "      <td>15</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>lunch</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>spend</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-876549d1-17b7-4f9e-a9ef-58d06a40849a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-876549d1-17b7-4f9e-a9ef-58d06a40849a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-876549d1-17b7-4f9e-a9ef-58d06a40849a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                 i        want          to         eat    chinese       food  \\\n",
              "i         4.666667  820.067066    0.987461    9.886364   0.757576   0.922330   \n",
              "want      2.327103    0.990339  601.375437    1.977301   5.341113   6.461136   \n",
              "to        2.333218    0.990350    4.936745  679.213794   2.274896   0.923360   \n",
              "eat       0.779778    0.990362    2.962103    0.988556  12.924921   2.772095   \n",
              "chinese   1.568812    0.990373    0.987387    0.988573   0.735961  76.740509   \n",
              "food     12.602574    0.990385   15.798504    0.988589   1.484444   4.603251   \n",
              "lunch     2.319692    0.990396    0.987422    0.988605   0.746184   1.842244   \n",
              "spend     1.550535    0.990408    1.974884    0.988621   0.752054   0.921772   \n",
              "\n",
              "             lunch       spend  \n",
              "i         0.870968    2.891892  \n",
              "want      5.236532    1.928216  \n",
              "to        6.112625  204.422486  \n",
              "eat      37.558695    0.963175  \n",
              "chinese   1.727793    0.963337  \n",
              "food      0.865561    0.963499  \n",
              "lunch     0.867489    0.963658  \n",
              "spend     0.869366    0.963817  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-087896a0-33f4-4aa5-aba2-bb2b90e02fc8\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>i</th>\n",
              "      <th>want</th>\n",
              "      <th>to</th>\n",
              "      <th>eat</th>\n",
              "      <th>chinese</th>\n",
              "      <th>food</th>\n",
              "      <th>lunch</th>\n",
              "      <th>spend</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>i</th>\n",
              "      <td>4.666667</td>\n",
              "      <td>820.067066</td>\n",
              "      <td>0.987461</td>\n",
              "      <td>9.886364</td>\n",
              "      <td>0.757576</td>\n",
              "      <td>0.922330</td>\n",
              "      <td>0.870968</td>\n",
              "      <td>2.891892</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>want</th>\n",
              "      <td>2.327103</td>\n",
              "      <td>0.990339</td>\n",
              "      <td>601.375437</td>\n",
              "      <td>1.977301</td>\n",
              "      <td>5.341113</td>\n",
              "      <td>6.461136</td>\n",
              "      <td>5.236532</td>\n",
              "      <td>1.928216</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>to</th>\n",
              "      <td>2.333218</td>\n",
              "      <td>0.990350</td>\n",
              "      <td>4.936745</td>\n",
              "      <td>679.213794</td>\n",
              "      <td>2.274896</td>\n",
              "      <td>0.923360</td>\n",
              "      <td>6.112625</td>\n",
              "      <td>204.422486</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>eat</th>\n",
              "      <td>0.779778</td>\n",
              "      <td>0.990362</td>\n",
              "      <td>2.962103</td>\n",
              "      <td>0.988556</td>\n",
              "      <td>12.924921</td>\n",
              "      <td>2.772095</td>\n",
              "      <td>37.558695</td>\n",
              "      <td>0.963175</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>chinese</th>\n",
              "      <td>1.568812</td>\n",
              "      <td>0.990373</td>\n",
              "      <td>0.987387</td>\n",
              "      <td>0.988573</td>\n",
              "      <td>0.735961</td>\n",
              "      <td>76.740509</td>\n",
              "      <td>1.727793</td>\n",
              "      <td>0.963337</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>food</th>\n",
              "      <td>12.602574</td>\n",
              "      <td>0.990385</td>\n",
              "      <td>15.798504</td>\n",
              "      <td>0.988589</td>\n",
              "      <td>1.484444</td>\n",
              "      <td>4.603251</td>\n",
              "      <td>0.865561</td>\n",
              "      <td>0.963499</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>lunch</th>\n",
              "      <td>2.319692</td>\n",
              "      <td>0.990396</td>\n",
              "      <td>0.987422</td>\n",
              "      <td>0.988605</td>\n",
              "      <td>0.746184</td>\n",
              "      <td>1.842244</td>\n",
              "      <td>0.867489</td>\n",
              "      <td>0.963658</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>spend</th>\n",
              "      <td>1.550535</td>\n",
              "      <td>0.990408</td>\n",
              "      <td>1.974884</td>\n",
              "      <td>0.988621</td>\n",
              "      <td>0.752054</td>\n",
              "      <td>0.921772</td>\n",
              "      <td>0.869366</td>\n",
              "      <td>0.963817</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-087896a0-33f4-4aa5-aba2-bb2b90e02fc8')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-087896a0-33f4-4aa5-aba2-bb2b90e02fc8 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-087896a0-33f4-4aa5-aba2-bb2b90e02fc8');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.6. Add-1 estimation is a blunt instrument\n",
        "- So add-1 isn't used for N-grams:\n",
        "  - We'll see better methods\n",
        "- But add-1 is used to smooth other NLP models\n",
        "  - For text classification\n",
        "  - In domains where the number of zeros isn't so huge"
      ],
      "metadata": {
        "id": "UMlKe9-zuNxe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Interpolation"
      ],
      "metadata": {
        "id": "chjes6Z4RKHL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.1. Backoff and Interpolation\n",
        "- Sometimes it helps to use **less** context\n",
        "  - Condtion on less context for contexts you haven't learned much about\n",
        "- Backoff:\n",
        "  - use trigram if you have good evidence\n",
        "  - otherwise bigram, otherwise unigram\n",
        "- Interpolation:\n",
        "  - mix unigram, bigram, trigram\n",
        "- Interpolation works better"
      ],
      "metadata": {
        "id": "driTodBX0ZSd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.2. Linear Interpolation\n",
        "- Simple interpolation: $$\\hat{P}(W_n|W_{i-1}W_{i-2}) = \\lambda_{1}P(W_n|W_{i-1}W_{i-2}) + \\lambda_{2}P(W_n|W_{i-1}) + \\lambda_{3}P(W_n)$$  \n",
        "$$\\sum_{i}{\\lambda_{i}} = 1$$\n",
        "\n",
        "- Lambdas conditional on context: $$\\hat{P}(W_n|W_{i-1}W_{i-2}) = \\lambda_{1}{(W_{i-1},W_{i-2})}P(W_n|W_{i-1}W_{i-2}) + \\lambda_{2}{(W_{i-1},W_{i-2})}P(W_n|W_{i-1}) + \\lambda_{3}{(W_{i-1},W_{i-2})}P(W_n)$$  "
      ],
      "metadata": {
        "id": "XZTmLBsZ1MA4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.3. How to set the lambdas?\n",
        "- Use a **held out** corpus(e.g.  dev set)\n",
        "- Choose lambdas to maximaze the probability of held-out data\n",
        "  - Fix the N-gram probabilities(on the training data)\n",
        "  - Then search for lambdas that give largest probability to held-out set: $$\\log{P}(W_{i}...W_{n}|M(\\lambda_{1}...\\lambda_{k})) = \\sum_{i}{\\log{P}_{M(\\lambda_{1}...\\lambda_{k})}(W_{i}|W_{i-1})}$$"
      ],
      "metadata": {
        "id": "gmr1j7ou46G5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.4. Unknown words: open vs closed vocabulary tasks\n",
        "- If we know all the words in advanced\n",
        "  - Vocabulary V is fixed\n",
        "  - Closed vocabulary task \n",
        "- Often we don't know this\n",
        "  - **Out of Vocabulary** = OOV words\n",
        "  - Open vocabulary task\n",
        "- Instead: create an unknown word token \\<UNK>\n",
        "  - Training of \\<UNK> probabilities\n",
        "    - Create a fixed lexicon L of size V\n",
        "    - At text normaliztion phase, any training word not in L changed to \\<UNK>\n",
        "    - Now we train its probabilities like a normal word\n",
        "  - At decoding time\n",
        "    - If text input: Use UNK probabilities for any word not in training\n"
      ],
      "metadata": {
        "id": "PUUvjg7a7fMG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.5. Huge web-scale N-grams\n",
        "- How to deal with, e.g., Google N-gram corpus\n",
        "- Pruning\n",
        "  - Only store N-grams with count > threshold\n",
        "    - Remove singletons of higher-order N-grams\n",
        "  - Entropy-based pruning\n",
        "- Efficiency\n",
        "  - Efficient data structures like tries\n",
        "  - Bloom filters: approximate language models\n",
        "  - Store words as indexed, not strings\n",
        "    - Use Huffman coding to fit large numbers of words into two bytes\n",
        "  - Quantize probabilities(4-8 bits instead of 8-byte float)"
      ],
      "metadata": {
        "id": "9_yrgjpY9iun"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.6. Smoothing for web-scale N-grams\n",
        "- \"Stupid backoff\"(Brants et al. 2007)\n",
        "- No discounting, Just relative frequencies"
      ],
      "metadata": {
        "id": "OhpnboqX_DiS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.7. N-gram smoothing Summary\n",
        "- Add-1 smoothing: \n",
        "  - OK for text categorization, not for language modeling\n",
        "- The most commonly used method:\n",
        "  - Extend Interpolated Kneser-Key\n",
        "- For very large N-grams like the web:\n",
        "  - Stupid backoff"
      ],
      "metadata": {
        "id": "SQmF0a3nAhWN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.8. Advanced Language Modeling\n",
        "- Discriminative models:\n",
        "  - Choose n-gram weights to improve a task, not to fit the training set\n",
        "- Parsing-based models\n",
        "- Caching Models\n",
        "  - Recently used words are more likely to appear\n",
        "  - These perform very poorly for speech recognition(why?)\n"
      ],
      "metadata": {
        "id": "aohTyqcCBCLF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Kneser Ney Smoothing\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-cbtc8_zNY2j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.1. Resulting Good-Turing numbers\n",
        "\n",
        "- Numbers from Church and Gale\n",
        "- 22 million words of AP Newswire\n",
        "\n",
        "$$c^* = \\frac{(c+1)N_{c+1}}{N_c}$$"
      ],
      "metadata": {
        "id": "XVVbpJ9iN9fX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.2. Absolute Discounting Interpolation\n",
        "- Save ourselves some time and just subtract 0.75 (or some d)!\n",
        "$$P_{Absolute Discounting}(W_i|W_{i-1}) = \\frac{c(W_{i-1},W_i)-d}{c(W_{i-1})} + \\lambda(w_{i-1})P(w)$$\n",
        "- (Maybe keeping a couple extra values of d for counts 1 and 2)\n",
        "- But should we really just use the regular unigram P(w)?"
      ],
      "metadata": {
        "id": "3geDttvLPBaZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.3. Kneser-Ney Smoothing\n",
        "- Better estimate for probabilities of lower-order unigrams!\n",
        "  - Shannon game: I can't without my reading ____!\n",
        "  - \"Francisco\" is more common than \"glasses\"\n",
        "  - ...but \"Francisco\" always follows \"San\"\n",
        "- The unigram is useful exactly when we haven't seen this bigram!\n",
        "- Instead of P(w): \"How likely is w\"\n",
        "- P<sub>continuation</sub>(w): \"How likely is w to appear as a novel continuation\"\n",
        "  - For each word, count the number of bigram types it completes\n",
        "  - Every bigram type was a novel continuation the first time it was seen\n",
        "- How many times does appear as a novel continuation\n",
        "$$P_{CONTINUATION}(w)\\propto |\\{w_{i-1}:c(w_{i-1},w) > 0\\}|$$\n",
        "- Normarlizes by the total number of word bigram types \n",
        "$$|\\{(w_{j-1}, w_j):c(w_{j-1},w_j) > 0\\}|$$\n",
        "$$P_{CONTINUATION}(w) = \\frac{|\\{w_{i-1}:c(w_{i-1},w) > 0\\}|}{|\\{(w_{j-1}, w_j):c(w_{j-1},w_j) > 0\\}|}$$\n",
        "\n",
        "- Alternative metaphor: The number of # of word types seen to precede w\n",
        "$$|\\{w_{i-1}:c(w_{i-1},w) > 0\\}|$$\n",
        "- normalized by the # of words preceding all words: \n",
        "$$P_{CONTINUATION}(w) = \\frac{|\\{w_{i-1}:c(w_{i-1},w) > 0\\}|}{\\sum_{w'}{|\\{w'_{i-1}:c(w'_{i-1},w'_i) > 0\\}|}}$$\n",
        "- A frequent word(Francisco) occurring in only one context (San) will have a low continuation probability\n",
        "$$P_{KN} = \\frac{max(c(w_{w-1}, w_i)-d,0)}{c(w_{i-1})}+\\lambda(w_{i-1})P_{CONTINUATION}(w_i)$$\n",
        "- lambda is a normalizing constant; the probability mass we've discounted \n",
        "$$\\lambda(w_{i-1}) = \\frac{d}{c(w_{i-1})}|\\{w:c(w_{i-1},w) > 0\\}|$$"
      ],
      "metadata": {
        "id": "yiyazTeqSc1e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Naive Bayes and Sentiment Classification"
      ],
      "metadata": {
        "id": "VtoKYmu2cPG7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. What is Text Classification"
      ],
      "metadata": {
        "id": "Oo1vZQUned-F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1. Who wrote which Federalist papers?\n",
        "- 1787-8: anonymous essays try to convince New York to ratify U.S Constitution: Jay, Madison, Hamilton\n",
        "- Authorship of 12 of the letter in dispute\n",
        "- 1963: solved by Mosteller and Wallace using Bayesian methods "
      ],
      "metadata": {
        "id": "GpucPrCcerdK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2. Male or female author"
      ],
      "metadata": {
        "id": "7HjUq3BBgq3P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3. Positive or negetive movie review?\n",
        "- unbelievably disappointing - **negative**\n",
        "- Full of zany characters and richly applied satire, and some great plot twists - **positive**\n",
        "- this is the greatest screwball comedy ever filmed - **positive**\n",
        "- It was pathetic. the worst part about it was the boxing scenes - **negative**"
      ],
      "metadata": {
        "id": "4WQydSB_g_Fr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.4. What is subject of this article? "
      ],
      "metadata": {
        "id": "Qn7KGA5PhbMn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.5. Text classification\n",
        "- Assigning subject categories, topics, or genres\n",
        "- Spam detection\n",
        "- Authorship identification\n",
        "- Age/gender identification\n",
        "- Language identification\n",
        "- Sentiment analysis"
      ],
      "metadata": {
        "id": "8xpUEAtCh0HR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.6. Text classification: definition\n",
        "- Input:\n",
        "  - a document **d**\n",
        "  - a fixed set of classes C = {c<sub>1</sub>,...,c<sub>j</sub>}\n",
        "- Output: a predicted class \n",
        "\n"
      ],
      "metadata": {
        "id": "XxkwOLLciJAA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.7. Classification Methods: Hand-coded rules\n",
        "- Rules based on combinations of words or other features\n",
        "  - spam: black-list-address OR(\"dollars\" AND \"have been selected\")\n",
        "- Accuracy can be high\n",
        "  - if rules carefully refined by expert\n",
        "- But building and maintaining these rules is expensive\n"
      ],
      "metadata": {
        "id": "8LyuH0s1j7Ee"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.8. Classification Methods: Supervised Machine Learning \n",
        "- Input:\n",
        "  - a document **d**\n",
        "  - a fixed set of classes C = {c<sub>1</sub>,...,c<sub>j</sub>}\n",
        "  - A training set of **m** hand-labeled documents (d<sub>1</sub>,c<sub>1</sub>),...,(d<sub>m</sub>,c<sub>m</sub>)\n",
        "- Output: a learned classifier\n",
        "- Any kind of classifier\n",
        "  - Naive Bayes\n",
        "  - Logistic regression\n",
        "  - support-vector machines\n",
        "  - k-nearest Neighbors... "
      ],
      "metadata": {
        "id": "MOX_4gczkq9j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. The Naive Bayes Classifier"
      ],
      "metadata": {
        "id": "OBwgmNPulz-m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1. Naive Bayes Intuition\n",
        "- Simple('naive') classification method based on Bayed rule\n",
        "- Relies on very simple representation of document -> **Bag of words**"
      ],
      "metadata": {
        "id": "jqCVSjJsLIxq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2. Bayes' Rule Applied to Documents and Classes\n",
        "- For a document **d** and a class **c**\n",
        "$$P(c|d) = \\frac{P(d|c)P(c)}{P(d)}$$"
      ],
      "metadata": {
        "id": "Z9af7ODcLzks"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3. Naive Bayes Classifier\n",
        "- $$c \\in C$$\n",
        "$$c_{MAP} = argmaxP(c|d)$$ \n",
        "$$c_{MAP} = argmax\\frac{P(d|c)P(c)}{P(d)}$$\n",
        "$$c_{MAP} = argmax{P(d|c)P(c)}$$ c끼리 비교할 때 계산식의 분모가 모두 같으므로 분모 생략 가능\n",
        "- Document d represented as features x1...xn\n",
        "$$c_{MAP} = argmax{P(x_1,...,x_n|c)P(c)}$$\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qjMPhDI1MfF7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.4. Multinomial Naive Bayes Independence Assumptions\n",
        "$$P(x_1,...,x_n|c)P(c)$$\n",
        "- **Bag of words assumption:** Assume position doesn't matter\n",
        "- **Conditional independence:** Assume the feature probabilities p(x<sub>i</sub>|c<sub>j</sub>) are independent given the class c (p(x<sub>i</sub>|c<sub>j</sub>)는 독립사상이어야 함)"
      ],
      "metadata": {
        "id": "u-OjL3e7HmuF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.5. Multinomial Naive Bayes Classifier\n",
        "$$c_{MAP} = argmax{P(x_1,...,x_n|c)P(c)}$$\n",
        "$$c_{NB} = argmax{P(c_j)\\prod_{x \\in X}P(x|c)}$$"
      ],
      "metadata": {
        "id": "Duv6h-BtLoxa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.6. Applying Multinomial Naive Bayes Classifiers to Text Classification\n",
        "- positions <- all word positions in test document\n",
        "$$c_{NB} = argmax{P(c_j)\\prod_{i \\in positions}P(x_i|c_j)}$$"
      ],
      "metadata": {
        "id": "kfmHIrDMMn3j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.7. Problems with multiplying lots of probs\n",
        "- There's a problem with this:\n",
        "$$c_{NB} = argmax{P(c_j)\\prod_{i \\in positions}P(x_i|c_j)}$$\n",
        "- Multiplying lots of probabilities can result in floating-point underflow!(확률은 곱할수록 값이 작아짐)\n",
        "- Idea: Use logs, because log(ab) = log(a) + log(b)\n",
        "- We'll sum logs of probabilities instead of multiplying probabilities!"
      ],
      "metadata": {
        "id": "_bNPTqcYNNRI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.8. We actually do everything in log space\n",
        "$$c_{NB} = argmax{P(c_j)\\prod_{i \\in positions}P(x_i|c_j)}$$\n",
        "$$c_{NB} = argmax\\left[\\log{P(c_j) + \\sum_{i \\in positions}\\log P(x_i|c_j)}\\right]$$\n",
        "- Taking log doesn't change the ranking of classes! The class with highest probability also has highest log probability!\n",
        "- It's a linear model: \n",
        "  - just a mass of a sum of weights: a **linear** function of the inputs \n",
        "  - So naive bayes is a **linear classifier**"
      ],
      "metadata": {
        "id": "vX4RDrCpOJ6v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Learning in Naive Bayes"
      ],
      "metadata": {
        "id": "HB-3TkUcQVA_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1. Learning the Multinomial Naive Bayes Model\n",
        "- First attempt: Maximum likelihood estimates\n",
        "  - Simply use the frequencies in the data\n",
        "  $$\\hat P(c_j) = \\frac{doccount(C = c_j)}{N_{doc}}$$\n",
        "  $$\\hat P(w_i|c_j) = \\frac{count(w_i, c_j)}{\\sum_{w\\in V} count(w, c_j)}$$"
      ],
      "metadata": {
        "id": "sBNZxe5gQYqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2. Parameter estimation\n",
        "- fraction of times word w<sub>i</sub> appears among all words in document of topic c<sub>j</sub>\n",
        "- Create mega-document for topic j by concatenating all docs in this topic\n",
        "  - Use frequency of w in mega-document"
      ],
      "metadata": {
        "id": "K902hfHKfGUC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3. Problem with Maximum Likelihood\n",
        "- What if we have seen no training documents with the word **fantastic** and classified in the topic **positive (thumbs-up)**?\n",
        "\n",
        "$$\\hat P(\"fantastic\"|positive) = \\frac{count(\"fantastic\", positive)}{\\sum_{w\\in V} count(w, positive)} = 0$$\n",
        "- Zero probabilities cannot be conditioned away, no matter the other evidence!\n",
        "$$c_{MAP} = argmax_c\\hat P(c) \\prod_i \\hat P(x_i|c)$$"
      ],
      "metadata": {
        "id": "J7eJ_x9Vg4e7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.4. Laplace(add-1) smoothing for Naive Bayes\n",
        "\n",
        "  $$\\hat P(w_i|c_j) = \\frac{count(w_i, c_j)+1}{\\sum_{w\\in V} (count(w, c_j)+1)} = \\frac{count(w_i, c_j)+1}{(\\sum_{w\\in V} count(w, c))+|V|}$$"
      ],
      "metadata": {
        "id": "5PoyEGEAjFnL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.5. Multinomial Naive Bayes: Learning \n",
        "- From training corpus, extract *Vocabulary*\n",
        "- Calculate P(c<sub>j</sub>) terms\n",
        "  - For each c<sub>j</sub> in C do \n",
        "    - docs<sub>j</sub> <- all docs with class = c<sub>j</sub>\n",
        "$$P(c_j) \\leftarrow \\frac{|docs_j|}{|total \\# documents|}$$\n",
        "- Calculate P(w<sub>k</sub>|c<sub>j</sub>) terms\n",
        "  - Text<sub>j</sub> <- single doc containing all docs<sub>j</sub>\n",
        "  - For each word w<sub>k</sub> in *Vocabulary*\n",
        "    - n<sub>k</sub> <- # of occurances of w<sub>k</sub> in text<sub>j</sub>\n",
        "$$P(w_k|c_j) \\leftarrow \\frac{n_k + \\alpha}{n + \\alpha|Vocabulary|}$$"
      ],
      "metadata": {
        "id": "m5jGXghXkZ8U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.6. Unknown words\n",
        "- What about unknown words\n",
        "  - That appear in our test data\n",
        "  - but not in our training data or vocabulary?\n",
        "- We **ignore** them \n",
        " - Remove them from the test document!\n",
        " - Pretend they weren't there\n",
        " - Don't include any probability for them at all!\n",
        "- Why don't we build an unknown word model?\n",
        "  - IT doesn't help: knowing which class has more unknown words is not generally helpfull "
      ],
      "metadata": {
        "id": "OsY-nbuIntRj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.7. Stop words\n",
        "- Some systems ignore stop words\n",
        "  - **Stop words:** very frequent words ex) the, a\n",
        "    - Sort the vocabulary by word frequency in training set\n",
        "    - Call the top 10 or 50 words the **stop word list**\n",
        "    - Remove all stop words from both training and test sets\n",
        "      - As if they were never there!\n",
        "- But removing stop words doesn't usually help\n",
        "  - So in practice most NB algorithms use **all** words and **don't** use stop word lists"
      ],
      "metadata": {
        "id": "RXbjFi9oZb7B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Sentiment and Binary NB"
      ],
      "metadata": {
        "id": "fEcqIeydaKcu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1. A worked sentiment example with add-1 smoothing\n",
        "- Training set:\n",
        "  - just plain boring -> -\n",
        "  - entirely predictable and lacks energe -> -\n",
        "  - no surprises and very few laughs -> -\n",
        "  - very powerful -> +\n",
        "  - the most fun film of the summer -> +\n",
        "- Test set:\n",
        "  - predictable with no fun -> ?\n",
        "- 1. Prior from training \n",
        "  - P(-) =  3/5, P(+) = 2/5\n",
        "- 2. Drop 'with'(training set에 존재하지 않는 단어이기 때문에)\n",
        "- 3. Likelihood from training(using add-1 smoothing)\n",
        "  - P('predictable'|-) = 1+1/14+20, P('predictable'|+) = 0+1/9+20\n",
        "  - P('no'|-) = 1+1/14+20, P('no'|+) = 0+1/9+20\n",
        "  - P('fun'|-) = 0+1/14+20, P('fun'|+) = 1+1/9+20\n",
        "- 4. Scoring the test set\n",
        "  - **P(-)P(S|-) = 3/5 * (2\\*2\\*1/34<sup>3</sup>) = 6.1 \\* 10<sup>-5</sup>**\n",
        "  - P(+)P(S|+) = 2/5 * (1\\*1\\*2/29<sup>3</sup>) = 3.2 \\* 10<sup>-5</sup>"
      ],
      "metadata": {
        "id": "X1IcLa5ZaVdZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2. Optimizing for sentiment analysis\n",
        "- For tasks like sentimant, word **occurrance** seems to be more important than word **frequency**\n",
        "  - The occurrence of the word *fantastic* tells us a lot\n",
        "  - The fact that it occurs 5 time may not tell us much more\n",
        "- **Binary multinomial naive bayes(binary NB)**\n",
        "  - Clip our word counts at 1"
      ],
      "metadata": {
        "id": "3-XqU4CFfas6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3. Binary multinomial naive bayes: Learning\n",
        "- From training corpus, extract *Vocabulary*\n",
        "- Calculate P(c<sub>j</sub>) terms\n",
        "  - For each c<sub>j</sub> in C do \n",
        "    - docs<sub>j</sub> <- all docs with class = c<sub>j</sub>\n",
        "$$P(c_j) \\leftarrow \\frac{|docs_j|}{|total \\# documents|}$$\n",
        "- Calculate P(w<sub>k</sub>|c<sub>j</sub>) terms\n",
        "  - Remove duplicates in each doc:\n",
        "    - For each word type w in doc<sub>j</sub>\n",
        "      - Retain only a single instance of w \n",
        "  - Text<sub>j</sub> <- single doc containing all docs<sub>j</sub>\n",
        "  - For each word w<sub>k</sub> in *Vocabulary*\n",
        "    - n<sub>k</sub> <- # of occurances of w<sub>k</sub> in text<sub>j</sub>\n",
        "$$P(w_k|c_j) \\leftarrow \\frac{n_k + \\alpha}{n + \\alpha|Vocabulary|}$$"
      ],
      "metadata": {
        "id": "gFI04z4TjYew"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.4. Binary multinomial naive bayes on a test document d\n",
        "- First remove all duplicate words from d\n",
        "- Then compute NB using the same equation\n",
        "$$c_{NB} = argmax{P(c_j)\\prod_{i \\in positions}P(x_i|c_j)}$$"
      ],
      "metadata": {
        "id": "k2U7QuLVj6CA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. More on Sentiment Classification "
      ],
      "metadata": {
        "id": "LtGOhshXM25o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.1. Sentiment Classification: Dealing with Negation\n",
        "- ex) I really like this movie. -> I really **don't** like this movie.\n",
        "- Negation changes the meaning of \"like\" to negative.\n",
        "- Negation can also change negative to positive-ish\n",
        "  - **don't** dismiss this film\n",
        "  - **doesn't** let us get bored\n",
        "- Simple baseline method: Add NOT_ to every word between negation and following punctuation\n",
        "  - ex) didn't like this movie, but I -> didn't NOT_like NOT_this NOT_movie, but I"
      ],
      "metadata": {
        "id": "0LhivA2JM6Du"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.2. Sentiment Classification: Lexicons\n",
        "- Sometimes we don't have enough labeled training data\n",
        "- In that case, we can make use of pre-built word lists Calles **lexicons**\n",
        "- There are various publically available lexicons\n",
        "  - ex) mpqa subjectivity lexicon, the general inquirer\n",
        "  "
      ],
      "metadata": {
        "id": "rRqE06sZO9bw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.3. Using Lexicons in Sentiment Classification\n",
        "- **Add a feature** that gets a count whenever a word from the lexicon occurs\n",
        " - E.g., a feature called \"**This word occurs in the positive lexicon**\" or \"**This word occurs in the negative lexicon**\"\n",
        "- Now all positive words(good, great, beautiful, wonderful) or negative words count for that feature\n",
        "- Using 1-2 features isn't as good as using all the words\n",
        "  - But when training data is sparse or not representative of the test set, dense lexicon features can help"
      ],
      "metadata": {
        "id": "82XRrT5KP-84"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.4. Naive Bayes in Other tasks: Spam Filtering\n",
        "- SpamAssassin Features:\n",
        "  - Mentions millions if (dollor)\n",
        "  - From: starts with many numbers\n",
        "  - Subject is all capitals\n",
        "  - HTML has a low ratio of text to image area\n",
        "  - \"One hundred percent guaranteed\"\n",
        "  - Claims you can be removed from the list"
      ],
      "metadata": {
        "id": "ozltLvCNRUZv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.5. Naive Bayes in Language ID\n",
        "- Determining what language a piece of text is written in.\n",
        "- Features based on character n-grams do very well\n",
        "- Important to train on lots of varieties of each language\n",
        "  - (e.g., American English varieties like African-American English, or English varieties around the world like Indian English)"
      ],
      "metadata": {
        "id": "SR71GfZUSB2J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.6. Summary: Naive Bayes is Not So Naive \n",
        "- Very Fast, low storage requirements\n",
        "- Work well with very small amounts of training data\n",
        "- Robust to Irrelevant Features\n",
        "  - Irrelevent Features cancel each other without affecting results\n",
        "- Very good in domains with many equally important features\n",
        "  - Decision Trees suffer from *fragmentation* in such cases - especially if little data\n",
        "- Optima if the independent assumption hold: If assumed independence is correct, then it is Bayes Optimal Classifier for probelm\n",
        "- A good dependence baseline for text classification\n",
        "  - But we will see other classifiers that give better accuracy"
      ],
      "metadata": {
        "id": "YFcNcTLrTtZA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Naive Bayes Relationship to Language Modeling"
      ],
      "metadata": {
        "id": "dmA2EktTPcBQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.1. Generative Model for Multinomial Naive Bayes\n"
      ],
      "metadata": {
        "id": "i5OQ6cuROP-f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.2. Naive Bayes and Language Modeling\n",
        "- Naive Bayes classifiers can use any sort of feature\n",
        "  - URL, email address, dictionaries, network features\n",
        "- But if, as in the previous sildes\n",
        "  - We use **only** word features\n",
        "  - We use **all** of the words in the text(not a subset)\n",
        "- Then\n",
        "  - Naive bayes has an important similarity to language modeling\n",
        "  "
      ],
      "metadata": {
        "id": "KjUH-NiN1pYP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.3. Each class = a unigram language model\n",
        "- Assigning each word: P(word | c)\n",
        "- Assigning each sentence: P(s|c) P(word1| c) * P(word2| c)..."
      ],
      "metadata": {
        "id": "VbVkZo-r4rvm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.4. Naive Bayes as a Language Model\n",
        "- Which class assigns the higher probability to s?\n",
        "- Model pos: I: 0.1/love: 0.1/this: 0.01/fun: 0.05/film: 0.1\n",
        "- Model neg: I: 0.2/love: 0.001/this: 0.01/fun: 0.005/film: 0.1\n",
        "- P(s|pos) > p(s|neg)"
      ],
      "metadata": {
        "id": "ABx_hdt85Lbe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Precision, Recall, and the F measure "
      ],
      "metadata": {
        "id": "AuL3odho_X7e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.1. The 2 by 2 contingency table\n",
        "referance: https://sumniya.tistory.com/26 \n",
        "\n",
        "||correct|not correct|\n",
        "|:---:|:---:|:---:|\n",
        "|selected|tp|fp|\n",
        "|not selected|fn|tn|\n"
      ],
      "metadata": {
        "id": "xefENcuR9ZUx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.2. Precision and recall\n",
        "\n",
        "- Accuracy: =  (true positives + true negative) / (true positives + false positives + true negetive + false negetive)\n",
        "- Precision: % of selected items that are correct = true positives / (true positives + false positives)\n",
        "- Recall: % of collected items that are selected = true positives / (true positives + false negatives)"
      ],
      "metadata": {
        "id": "vquTlbYL-7Ob"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.3. A combined measure: F\n",
        "- A combined measere that assesses the P/P tradeoff is F measure(weight harmonic mean)\n",
        "$$F = \\frac{1}{\\alpha \\frac{1}{P}+(1-\\alpha) \\frac{1}{R}} = \\frac{(\\beta^2+1)PR}{\\beta^2P+R}$$\n",
        "- The harmonic mean is a very conservative average\n",
        "- People usually use balanced F1 measure\n",
        "  - i.e.. with beta = 1 (that is. alpha = 1/2): F = 2PR/(P+R)"
      ],
      "metadata": {
        "id": "KrfBOL6EAWly"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Text Classification Evaluation"
      ],
      "metadata": {
        "id": "QgoK-hZa9VMh"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "CS 124.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "wZ31V82xy0Mh",
        "e6Fj0pcJzS6-",
        "lbB4VHZqzqQf",
        "EkT0NsmyzshX",
        "uuERJpxbzuWS",
        "Uj2ka8Cmzw0c",
        "YGS65WFwz30W",
        "O-RH80G1z2Xu",
        "DmnQYOHCz8Zl",
        "-2wA_7o1z-44",
        "WJ8_dOk00Amp",
        "MnWC_e3J0aeV",
        "RQ2Xoqti0f5x",
        "RtMyrutm0hr1",
        "57eNmqF20jzD",
        "cqrzowxA0oTb",
        "81ohUAO10p2n",
        "6qxTMcwH0r6T",
        "vG07CRx70wNX",
        "D6bpFLoU050E",
        "SD3_YmWv09LV",
        "7_bsmIsi0_Ij",
        "hvIlSRM01Akl",
        "OTwW_Wx61NYR",
        "34GK_AKO1OXF",
        "emkCPuGm1P5V",
        "trwSkWzZ1UT8",
        "QR5e_ATe1aV1",
        "loppGyF51bOP",
        "gX2mb4NW1bQ0",
        "xFOfpMb91bTt",
        "CkfS592f1bWW",
        "_dgsUuly1bZP",
        "sG0j1FJK1bb0",
        "FPle5Ejr1beO",
        "SLLrXGbF1bgx",
        "ZTZ7cIWG1byD",
        "rmzlGU5D1b0n",
        "YZB24rN-1b3G",
        "s7b5L6Uy1b5p",
        "uRtFHBc-11LE",
        "JcE4LYcu11UF",
        "ckuYnga011bZ",
        "hmfo6-MA11jr",
        "-_9HPbR011ot",
        "QRsiAe-g3f_V",
        "Ky7dbiLU4oSh",
        "m6HTMKqRq81A",
        "XYXs3ApPsZkX",
        "6juVBhNxlyoz",
        "IPLHWypznJ1Z",
        "idLRfl1ynXsz",
        "kgnrVynZn0k9",
        "ajr5ioLdoPFi",
        "_ITDen2doVFb",
        "CvQyeMrmpPkT",
        "e7WK3JWPwK4G",
        "Yjh_amjvyxw3",
        "Eh-Zwaq2zL-0",
        "R2pKLAHb5hqy",
        "--F4XpSj8MQ9",
        "JUiFuxUd-Gq-",
        "Jk6ABmHN_eui",
        "GVUMw5JXBJae"
      ],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}